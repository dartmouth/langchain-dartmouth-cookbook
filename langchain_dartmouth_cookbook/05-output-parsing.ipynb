{
   "cells": [
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "# Output parsing\n",
            "\n",
            "We often use LLMs as text processing engines to convert unstructured text into structured data. In this recipe, we will show how we can use an LLM to extract a structured timeline from a narrative paragraph containing years."
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Input data\n",
            "\n",
            "We will use the following text, which is a paragraph on the history of the Baker-Berry Library from [Wikipedia](https://en.wikipedia.org/wiki/Baker-Berry_Library):  "
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "unstructured_text = \"\"\"The original, historic library building is the Fisher Ames Baker Memorial Library; it opened in 1928 with a collection of 240,000 volumes. The building was designed by Jens Fredrick Larson, modeled after Independence Hall in Philadelphia, and funded by a gift to Dartmouth College by George Fisher Baker in memory of his uncle, Fisher Ames Baker, Dartmouth class of 1859. The facility was expanded in 1941 and 1957â€“1958 and received its one millionth volume in 1970.\n",
            "\n",
            "In 1992, John Berry and the Baker family donated US $30 million for the construction of a new facility, the Berry Library designed by architect Robert Venturi, adjoining the Baker Library. The new complex, the Baker-Berry Library, opened in 2000 and was completed in 2002.[6] The Dartmouth College libraries presently hold over 2 million volumes in their collections.\"\"\"\n",
            "\n",
            "print(unstructured_text)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "As we can see, the history is written in a narrative style. It mentions various important points in the history of Baker-Berry, but not in a way that they can be easily extracted. There are two major challenges here: \n",
            "- Not all years in the text are actually relevant to the task (_\"class of 1859\"_)\n",
            "- Each year needs a succinct summary of the corresponding event\n",
            "\n",
            "We can solve both of these challenges with an LLM."
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Data extraction\n",
            "\n",
            "The most straight-forward approach is to simply prompt a model to extract the timeline of events from the unstructured text. Let's go ahead and try that first! \n",
            "\n",
            "We will start by using what we have learned in previous recipes about instantiating and invoking a chat model:"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "from langchain_dartmouth.llms import ChatDartmouth\n",
            "from dotenv import find_dotenv, load_dotenv\n",
            "\n",
            "load_dotenv(find_dotenv())"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### Basic approach"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "llm = ChatDartmouth(model_name=\"llama-3-1-8b-instruct\")\n",
            "\n",
            "prompt = (\n",
            "    \"Extract a succinct timeline of events directly related the Library from the following text: \\n\\n\"\n",
            "    + unstructured_text\n",
            ")\n",
            "\n",
            "response = llm.invoke(prompt)\n",
            "\n",
            "print(response.content)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "It worked! Well, kind of. The goal was to extract the data as a _data structure_ (like a Python `list`, or  a `dict`), not just another string. We could now manually parse the response, but if you run the above cell multiple times, you will see that the model may produce different formats of the timeline. This makes it hard to write code that can reliably and reproducibly extract the data. Let's take it one step further and instruct the model to return the data in a specific format."
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### Explicit output structure\n",
            "\n",
            "Ideally, we want the data to be a `list` of `dict` objects, where each `dict` has two fields: `'year'` and `'event'`. The LLM can't return actual Python objects, it can only ever return a string. But let's instruct our model to format the string in such a way that we can parse it back into an object in Python. [JSON   ](https://en.wikipedia.org/wiki/JSON) is a great format for this, because it is string-based and can be easily parsed by [Python's `json` module](https://docs.python.org/3/library/json.html). Here is how we could modify our prompt:"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "prompt = (\n",
            "    \"Extract a succinct timeline of events directly related the Library from the following text. Return the timeline as a list of dictionaries, where each dictionary has two keys: 'year' and 'event'. Format your output in JSON format. The text:\\n\\n\"\n",
            "    + unstructured_text\n",
            ")"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "Now let's give it another try:"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "response = llm.invoke(prompt)\n",
            "\n",
            "print(response.content)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "Close! But if you run the cell multiple times, you will see that the model adds additional stuff around the JSON string: Markdown tags for a code block, or sometimes a brief preamble or closing summary. However: Many LLMs, including Llama 3.1, are trained to use the markdown codeblock (everything between the triple backticks ` ``` `) around the actual data. So we could write a parser that first extracts the text between those backticks, and then parses that string as a JSON.\n",
            "\n",
            "Lucky for us, LangChain includes [such a parser](https://api.python.langchain.com/en/latest/output_parsers/langchain_core.output_parsers.json.JsonOutputParser.html) already! Since `langchain_dartmouth` is built on LangChain, we can directly use that component with `ChatDartmouth`!\n",
            "\n",
            "By convention, most components in LangChain use the `invoke` method to \"do their thing\", so here is how we can fit everything together:"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "from langchain_core.output_parsers import JsonOutputParser\n",
            "\n",
            "parser = JsonOutputParser()\n",
            "\n",
            "response = llm.invoke(prompt)\n",
            "timeline = parser.invoke(response)\n",
            "\n",
            "for event in timeline:\n",
            "    print(event)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "There are [many more output parsers](https://api.python.langchain.com/en/latest/core_api_reference.html#module-langchain_core.output_parsers) available in LangChain for all sorts of different desired output formats. All of them have the same usage pattern demonstrated above: Instruct the model to return the data in a specific format, then pass the model's response through the parser.\n",
            "\n",
            "If there is a specific format you need that is not already supported by any of the available parsers, you can also write your own by subclassing any of them. Let's say instead of a generic JSON, we wanted to extract a [Pandas](https://pandas.pydata.org/docs/index.html) `DataFrame`. We could create such a parser by subclassing the `JsonOutputParser` and adding an additional step to its `invoke` method:\n",
            "\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "import pandas as pd\n",
            "\n",
            "\n",
            "class DataFrameParser(JsonOutputParser):\n",
            "    def invoke(self, text: str) -> pd.DataFrame:\n",
            "        json_data = super().invoke(text)\n",
            "        return pd.DataFrame.from_records(json_data)\n",
            "\n",
            "\n",
            "parser = DataFrameParser()\n",
            "response = llm.invoke(prompt)\n",
            "df = parser.invoke(response)\n",
            "df"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Summary\n",
            "\n",
            "In this recipe, we saw that LLMs are great to extract structured data from unstructured text. Since LLMs can only output strings, output parsers are a great tool to convert the text representation of the structured data into Python objects (like lists, dictionaries, or even data frames) for further processing."
         ]
      }
   ],
   "metadata": {
      "kernelspec": {
         "display_name": ".venv",
         "language": "python",
         "name": "python3"
      },
      "language_info": {
         "codemirror_mode": {
            "name": "ipython",
            "version": 3
         },
         "file_extension": ".py",
         "mimetype": "text/x-python",
         "name": "python",
         "nbconvert_exporter": "python",
         "pygments_lexer": "ipython3",
         "version": "3.12.6"
      }
   },
   "nbformat": 4,
   "nbformat_minor": 2
}