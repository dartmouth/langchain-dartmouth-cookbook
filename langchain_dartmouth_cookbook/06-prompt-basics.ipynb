{
   "cells": [
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "# Prompt basics\n",
            "\n",
            "Prompts are strings that guide the LLM to generate a response. So far, we have used basic Python strings for prompts. The LangChain ecosystem, and by extension `langchain_dartmouth`, offers more advanced ways of constructing prompts of different types to fit different use cases. This recipe explores the two fundamental types of LLM inputs: Basic prompts and messages.\n",
            "\n",
            "```{hint}\n",
            "It's important to keep in mind that no matter what type of prompt you use, eventually everything will be converted into a string that is sent to the LLM. All LLMs process strings as input and generate strings as output. The advanced types facilitate more expressive, concise, or efficient code, but theoretically, you could replace any advanced prompt type with a basic string-based one. Learning how to work with advanced prompts will help make your code much easier to understand, expand, and maintain, however.\n",
            "```"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "from dotenv import find_dotenv, load_dotenv\n",
            "\n",
            "load_dotenv(find_dotenv())"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Basic prompts\n",
            "\n",
            "Just like we did in previous recipes, we can use simple strings as prompts for both completion and chat models:"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "from langchain_dartmouth.llms import DartmouthLLM, ChatDartmouth\n",
            "\n",
            "llm = DartmouthLLM(model_name=\"codellama-13b-python-hf\", return_full_text=True)\n",
            "chat_model = ChatDartmouth(model_name=\"llama-3-2-11b-vision-instruct\")\n",
            "\n",
            "print(llm.invoke(\"def fibonacci(x):\"))\n",
            "print(\"-\" * 10)\n",
            "\n",
            "chat_model.invoke(\"Write a haiku about Python.\")"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "Since they are just strings, we can build basic prompts using Python's standard string manipulation functions. For example, we can use a variable in our prompt using Python's f-string syntax:"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "topic = \"dogs\"\n",
            "\n",
            "prompt = f\"Tell me a joke about {topic}\"\n",
            "\n",
            "chat_model.invoke(prompt)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "For a more advanced way to build a prompt with placeholders, check out the next [recipe on Prompt Templates](07-prompt-templates.ipynb)!\n",
            "\n",
            "While the models can handle simple strings as prompts, we notice that the chat model returns a more complex object `AIMessage`. This is an example of the second type of prompts in LangChain: Messages."
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Messages\n",
            "\n",
            "Messages are a collection of classes that are designed around the concept of a back-and-forth conversation, where each conversational turn is represented by a message object. This aligns well with how chat models are trained, which are fine-tuned on conversations that are broken down into conversational turns using a chat template. Here is an example of a couple of turns for Llama 3:\n",
            "\n",
            "> <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            ">\n",
            ">You are a helpful AI assistant for travel tips and recommendations<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            ">\n",
            ">What is France's capital?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            ">\n",
            ">Bonjour! The capital of France is Paris!<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            ">\n",
            ">What can I do there?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            ">\n",
            ">Paris, the City of Light, offers a romantic getaway with must-see attractions like the Eiffel Tower and Louvre Museum, romantic experiences like river cruises and charming neighborhoods, and delicious food and drink options, with helpful tips for making the most of your trip.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            ">\n",
            ">Give me a detailed list of the attractions I should visit, and time it takes in each one, to plan my trip accordingly.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "Let's focus on some of the individual parts:\n",
            "\n",
            "- There are different headers marked by `<start_header_id>` and `<end_header_id>`. These identify the role of the following message, which ends with `<eot_id>` (_eot_ stands for _end of turn_).\n",
            "- The roles in this conversation are `system`, `user` and `assistant`.\n",
            "\n",
            "Through this markup, the conversation can be understood as a sequence of messages from different \"speakers\". This structure is the same for most chat models, not just Llama 3, although the formatting for the markup may differ.\n",
            "\n",
            "LangChain builds on this structure and provides various messages that can be conveniently composed and sequenced to form such a conversation. Each message consists of a role specificer and the actual content. The most important message types are:\n",
            "\n",
            "- `ChatMessage`: A message that can be assgined an arbitrary role and content.\n",
            "- `SystemMessage`: A message with a hardcoded role of `\"System\"`, but assignable content.\n",
            "- `HumanMessage`: A message with a hardcoded role of `\"Human\"`, but assignable content.\n",
            "- `AIMessage`: A message with a hardcoded role of `\"AI\"`, but assignable content.\n",
            "\n",
            "Let's explore these messages a little further.\n",
            "\n",
            "As we can see in the following example, a `ChatMessage` with the corresponding role specifier is functionally equivalent to the more specialized messages:"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "from langchain_core.messages import ChatMessage, HumanMessage, SystemMessage, AIMessage\n",
            "\n",
            "\n",
            "chat_system_message = ChatMessage(\n",
            "    role=\"System\",\n",
            "    content=\"You are a helpful AI assistant. Always be polite and end every response with a pun.\",\n",
            ")\n",
            "system_message = SystemMessage(\n",
            "    \"You are a helpful AI assistant. Always be polite and end every response with a pun.\"\n",
            ")\n",
            "\n",
            "chat_human_message = ChatMessage(role=\"Human\", content=\"How are you doing today?\")\n",
            "human_message = HumanMessage(content=\"How are you doing today?\")\n",
            "\n",
            "chat_ai_message = ChatMessage(\n",
            "    role=\"AI\", content=\"I am doing fine, thanks. How about you?\"\n",
            ")\n",
            "ai_message = AIMessage(content=\"I am doing fine, thanks. How about you?\")\n",
            "\n",
            "print(chat_system_message)\n",
            "print(system_message)\n",
            "print(\"-\" * 10)\n",
            "\n",
            "print(chat_human_message)\n",
            "print(human_message)\n",
            "print(\"-\" * 10)\n",
            "\n",
            "print(chat_ai_message)\n",
            "print(ai_message)\n",
            "print(\"-\" * 10)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "For the specialized messages, the role is not explicitly stated because it is defined by each message's type. \n",
            "\n",
            "When we string several of these messages together, we are creating a conversation:"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "conversation = [system_message, human_message, ai_message]\n",
            "for msg in conversation:\n",
            "    print(msg)\n",
            "print(\"-\" * 10)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "We can pass a sequence of messages to a chat model and the model will continue the conversation:"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "print(chat_model.invoke(conversation))"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "Note that it is not required to have a strict human-ai-human back-and-forth in the conversation. Even though the last message was an AI message, the model continues the conversation just fine!\n",
            "\n",
            "In case you are wondering where the necessary formatting is applied to the messages: This is actually handled by the model serving backend that is running on Dartmouth servers. This architecture allows us to deploy new models, which may require new chat formatting, in Dartmouth's cloud and have them be immediately available through `langchain_dartmouth`, without requiring an update of the library.\n",
            "\n",
            "If, however, you ever need a transcript-style string representation of a sequence of messages, you can use a utility function from LangChain called `get_buffer_string`:"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "from langchain_core.messages.utils import get_buffer_string\n",
            "\n",
            "print(get_buffer_string([system_message, chat_human_message, ai_message]))"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "Another benefit of messages is that they greatly facilitate managing a conversation history. Check out the [recipe on conversational memory](09-conversational-memory.ipynb) to learn more!"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Multimodal prompts\n",
            "\n",
            "Some LLMs also support image input, also known as vision capability. `langchain_dartmouth` offers support for multimodal prompts through the parent project [LangChain](https://python.langchain.com/docs/concepts/multimodality/).\n",
            "\n",
            "```{note}\n",
            "Multimodal prompts are only supported by chat models. You can, however, use either on-premise models through `ChatDartmouth`, or third-party models through `ChatDartmouthCloud`.\n",
            "```\n",
            "\n",
            "You can check whether a model is vision-capable using the `list()` method (as described in [Large Language Models](03-llms.ipynb)):"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Find a vision capable model\n",
            "for model in ChatDartmouth.list():\n",
            "    if \"vision\" in model[\"capabilities\"]:\n",
            "        vision_model_spec = model\n",
            "        break\n",
            "\n",
            "\n",
            "vision_model = ChatDartmouth(\n",
            "    model_name=vision_model_spec[\"name\"],\n",
            "    # Set seed and temperature for reproducibility\n",
            "    seed=42,\n",
            "    temperature=0,\n",
            ")"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "Let's ask this model to describe the logo of the cookbook:\n",
            "\n",
            "<img src=\"_static/img/langchain_dartmouth-cookbook-logo-light.png\" alt=\"cookbook log\" width=\"200\"/>\n",
            "\n",
            "To present the image to the model, we need to first transform it into a text representation using [Base64 encoding](https://en.wikipedia.org/wiki/Base64)."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 10,
         "metadata": {},
         "outputs": [],
         "source": [
            "import base64\n",
            "\n",
            "image_path = \"_static/img/langchain_dartmouth-cookbook-logo-light.png\"\n",
            "with open(image_path, \"rb\") as image_file:\n",
            "    image_data = base64.b64encode(image_file.read()).decode(\"utf-8\")"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "```{hint}\n",
            "\n",
            "`base64.b64encode()` by itself returns a [byte string](https://stackoverflow.com/questions/6224052/what-is-the-difference-between-a-string-and-a-byte-string). We want the image data represented by a regular string, however, that's why need to add the call to `decode(\"utf-8\")`.\n",
            "```"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "Now we can create a prompt that contains the image data. We can use a `ChatMessage` or a `HumanMessage`, as before, but note that instead of just passing a simple string as the content, we now pass a list of dictionaries. Each dictionary describes a different part of the prompt. \n",
            "\n",
            "```{hint}\n",
            "You could also send multiple images at once. If you want the model to reference more than one image, simply add more dictionaries to the list!\n",
            "```"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "message = HumanMessage(\n",
            "    content=[\n",
            "        {\n",
            "            \"type\": \"text\",\n",
            "            \"text\": \"Describe this image\",\n",
            "        },\n",
            "        {\n",
            "            \"type\": \"image_url\",\n",
            "            \"image_url\": {\"url\": f\"data:image/png;base64,{image_data}\"},\n",
            "        },\n",
            "    ],\n",
            ")\n",
            "response = vision_model.invoke([message])\n",
            "response.pretty_print()"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "```{hint}\n",
            "As you may have guessed from the name: You can also pass URLs of images hosted on the web directly to the model instead of using a local image.\n",
            "```"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Summary\n",
            "\n",
            "LLMs process strings as input and produce strings as output. Prompts can therefore be represented as simple strings. However, using more specialized data structures based on messages can make the code more readable, concise, and easier to extend. In this recipe, we explored some of the messages implemented in LangChain and saw how they can be used to manage a conversation."
         ]
      }
   ],
   "metadata": {
      "kernelspec": {
         "display_name": ".venv",
         "language": "python",
         "name": "python3"
      },
      "language_info": {
         "codemirror_mode": {
            "name": "ipython",
            "version": 3
         },
         "file_extension": ".py",
         "mimetype": "text/x-python",
         "name": "python",
         "nbconvert_exporter": "python",
         "pygments_lexer": "ipython3",
         "version": "3.12.9"
      }
   },
   "nbformat": 4,
   "nbformat_minor": 2
}
