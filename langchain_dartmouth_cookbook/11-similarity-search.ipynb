{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import find_dotenv, load_dotenv\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several benefits to having the embedding of a word, a primary one is that it gives us the ability to compare how close two words are in meaning. One way of simpling doing so is by taking the **dot product**.\n",
    "\n",
    "The similarity between two embeddings is given by their dot product. \n",
    "\n",
    "$$\n",
    "\\text{Similarity} = \\vec{v} \\cdot \\vec{w}\n",
    "$$\n",
    "\n",
    "First let embed some words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_dartmouth.embeddings import DartmouthEmbeddings\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "embeddings = DartmouthEmbeddings()\n",
    "text_1 = \"Japan\"\n",
    "text_2 = \"Sushi\"\n",
    "text_3 = \"Italy\"\n",
    "text_4 = \"Pizza\"\n",
    "\n",
    "embed_1 = embeddings.embed_query(text_1)\n",
    "embed_2 = embeddings.embed_query(text_2)\n",
    "embed_3 = embeddings.embed_query(text_3)\n",
    "embed_4 = embeddings.embed_query(text_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's do the dot product\n",
    "$$\n",
    "v\\cdot w = \\sum_{i = 1}^N(v_i \\cdot w_i)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between Japan and Sushi is 0.6915366834825497\n"
     ]
    }
   ],
   "source": [
    "N = len(embed_1)\n",
    "similarity_1_2 = []\n",
    "\n",
    "for i in range(N):\n",
    "    v = embed_1[i]\n",
    "    w = embed_2[i]\n",
    "    similarity_1_2.append(v * w)    \n",
    "    \n",
    "print(f'Similarity between {text_1} and {text_2} is {sum(similarity_1_2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's repeat this for the other words too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between Japan and Italy is 0.7488653578639409\n",
      "Similarity between Japan and Pizza is 0.5895998995258541\n",
      "Similarity between Sushi and Italy is 0.6092699421928527\n",
      "Similarity between Sushi and Pizza is 0.780035362735349\n",
      "Similarity between Italy and Pizza is 0.7009643386868145\n"
     ]
    }
   ],
   "source": [
    "similarity_1_3 = []\n",
    "similarity_1_4 = []\n",
    "similarity_2_3 = []\n",
    "similarity_2_4 = []\n",
    "similarity_3_4 = []\n",
    "\n",
    "for i in range(length_of_vector): \n",
    "    similarity_1_3.append(embed_1[i] * embed_3[i])\n",
    "    similarity_1_4.append(embed_1[i] * embed_4[i])\n",
    "    similarity_2_3.append(embed_2[i] * embed_3[i])\n",
    "    similarity_2_4.append(embed_2[i] * embed_4[i])\n",
    "    similarity_3_4.append(embed_3[i] * embed_4[i])\n",
    "\n",
    "print(f'Similarity between {text_1} and {text_3} is {sum(similarity_1_3)}')\n",
    "print(f'Similarity between {text_1} and {text_4} is {sum(similarity_1_4)}')\n",
    "print(f'Similarity between {text_2} and {text_3} is {sum(similarity_2_3)}')\n",
    "print(f'Similarity between {text_2} and {text_4} is {sum(similarity_2_4)}')\n",
    "print(f'Similarity between {text_3} and {text_4} is {sum(similarity_3_4)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here we can see that Japan and Sushi are are about as similar as Italy and Pizza. Italy and Sushi and Japan and Pizza are also equally similar. We can see that Japan and Italy are very similar, perhaps because they are both countries. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "**Note:** This is a source of where *bias* comes in for machine learning models. These results do not mean that you can't get good sushi in Italy or good pizza in Japan. It simply means that in the training data for this embedding model, these words generally appeared close to one another. \n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing an embedding\n",
    "A better way to understand an embedding is to visualize it. Let's generate some random words related to different domains, and find their embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import umap\n",
    "from langchain_dartmouth.llms import ChatDartmouth\n",
    "\n",
    "\n",
    "llm = ChatDartmouth(model_name=\"llama-3-1-8b-instruct\", seed=42, temperature=0.0)\n",
    "parser = JsonOutputParser()\n",
    "\n",
    "chain = llm | parser\n",
    "\n",
    "response = chain.invoke(\n",
    "    \"Generate 30 different words that are well-suited to showcase how word embeddings work. \"\n",
    "    \"Draw the words from domains like animals, finance, and food. The food one should contain tomato \"\n",
    "    \"Return the words in JSON format, using the domain as the key, and the words as values. \"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = pd.DataFrame.from_dict(response).melt(var_name=\"domain\", value_name=\"word\")\n",
    "\n",
    "embeddings = DartmouthEmbeddings(model_name=\"bge-large-en-v1-5\")\n",
    "words[\"embedding\"] = embeddings.embed_documents(words[\"word\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "**Note:** It is difficult to visualize a 1024 dimensional vector, as we're not 1024 dimensional humans! One way to get around this is by using a [UMAP](https://umap-learn.readthedocs.io/en/latest/) (Uniform Manifold Approximation and Projection) to represent this large vector as a 2 dimesional one. \n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'words' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mumap\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m embeddings_list \u001b[38;5;241m=\u001b[39m \u001b[43mwords\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membedding\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto_list()\n\u001b[0;32m      4\u001b[0m mapper \u001b[38;5;241m=\u001b[39m umap\u001b[38;5;241m.\u001b[39mUMAP()\u001b[38;5;241m.\u001b[39mfit(embeddings_list)\n\u001b[0;32m      5\u001b[0m umap_embeddings \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(mapper\u001b[38;5;241m.\u001b[39mtransform(embeddings_list), columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUMAP_x\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUMAP_y\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'words' is not defined"
     ]
    }
   ],
   "source": [
    "import umap\n",
    "\n",
    "embeddings_list = words[\"embedding\"].to_list()\n",
    "mapper = umap.UMAP().fit(embeddings_list)\n",
    "umap_embeddings = pd.DataFrame(mapper.transform(embeddings_list), columns=[\"UMAP_x\", \"UMAP_y\"])\n",
    "\n",
    "words = pd.concat([words, umap_embeddings], axis=1)\n",
    "\n",
    "words.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a scatter plot using matplotlib\n",
    "for i in words[\"domain\"].unique():\n",
    "    subset = words[words[\"domain\"] == i]\n",
    "    plt.scatter(subset[\"UMAP_x\"], subset[\"UMAP_y\"], label=i)\n",
    "    \n",
    "    # Add the text labels\n",
    "    for j in range(len(subset)):\n",
    "        plt.text(\n",
    "            subset[\"UMAP_x\"].iloc[j],\n",
    "            subset[\"UMAP_y\"].iloc[j] + 0.12,\n",
    "            subset[\"word\"].iloc[j],\n",
    "            horizontalalignment=\"center\",\n",
    "            verticalalignment=\"center\",\n",
    "            fontsize=6,\n",
    "        )\n",
    "\n",
    "# Add legend and show plot\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.title(\"UMAP Projection of Words\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that groups with words related to foods, and animals, and finance are somewhat close to each other. This let's us find the similarity between different words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "This recipe showed how to find the similarity between two embeddings. Visualizing embeddings can be a good way to represent this similarity. UMAP can be used to represent high-dimensional embeddings in a 2D plane, so we can easily observe it. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
