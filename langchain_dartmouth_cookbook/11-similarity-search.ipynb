{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several benefits to having the embedding of a word, a primary one is that it gives us the ability to compare how close two words are in meaning. One way of simpling doing so is by taking the **dot product**.\n",
    "The similarity between two embeddings is given by their dot product. \n",
    "\n",
    "$$\n",
    "\\text{Similarity} = \\vec{v} \\cdot \\vec{w}\n",
    "$$\n",
    "\n",
    "First let embed some words, which we learned in the previous cookbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import find_dotenv, load_dotenv\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_dartmouth.embeddings import DartmouthEmbeddings\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "embeddings = DartmouthEmbeddings()\n",
    "text_1 = \"Japan\"\n",
    "text_2 = \"Sushi\"\n",
    "text_3 = \"Italy\"\n",
    "text_4 = \"Pizza\"\n",
    "\n",
    "embed_1 = embeddings.embed_query(text_1)\n",
    "embed_2 = embeddings.embed_query(text_2)\n",
    "embed_3 = embeddings.embed_query(text_3)\n",
    "embed_4 = embeddings.embed_query(text_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's do the dot product\n",
    "$$\n",
    "v\\cdot w = \\sum_{i = 1}^N(v_i \\cdot w_i)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(embed_1)\n",
    "similarity_1 = []\n",
    "\n",
    "for i in range(N):\n",
    "    v = embed_1[i]\n",
    "    w = embed_2[i]\n",
    "    similarity_1.append(v * w)    \n",
    "    \n",
    "print(f'Similarity between {text_1} and {text_2} is {sum(similarity_1)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's repeat this for the other words too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_2 = []\n",
    "similarity_3 = []\n",
    "similarity_4 = []\n",
    "similarity_5 = []\n",
    "similarity_6 = []\n",
    "\n",
    "for i in range(N): \n",
    "    similarity_2.append(embed_1[i] * embed_3[i])\n",
    "    similarity_3.append(embed_1[i] * embed_4[i])\n",
    "    similarity_4.append(embed_2[i] * embed_3[i])\n",
    "    similarity_5.append(embed_2[i] * embed_4[i])\n",
    "    similarity_6.append(embed_3[i] * embed_4[i])\n",
    "\n",
    "print(f'Similarity between {text_1} and {text_3} is {sum(similarity_2)}')\n",
    "print(f'Similarity between {text_1} and {text_4} is {sum(similarity_3)}')\n",
    "print(f'Similarity between {text_2} and {text_3} is {sum(similarity_4)}')\n",
    "print(f'Similarity between {text_2} and {text_4} is {sum(similarity_5)}')\n",
    "print(f'Similarity between {text_3} and {text_4} is {sum(similarity_6)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this, we observe that **Japan** and *Sushi* share a similarity comparable to that of **Italy** and *Pizza*. Likewise, **Italy** and *Sushi* as well as **Japan** and *Pizza* exhibit similar levels of association. Interestingly, **Japan** and **Italy** also demonstrate a high degree of similarity, likely because both are countries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "**Note:** This is a source of where *bias* comes in for machine learning models. These results do not mean that you can't get good sushi in Italy or good pizza in Japan. It simply means that in the training data for this embedding model, these words generally appeared close to one another. \n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Similarity\n",
    "A better way to understand an embedding is to visualize it. Let's generate some random words related to different domains, and find their embeddings. In the [building chains](./08-building-chains.ipynb) notebook, the idea of a pipeline was introduced. We use this to generate and parse an output from the llm as a test embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import umap\n",
    "from langchain_dartmouth.llms import ChatDartmouth\n",
    "\n",
    "\n",
    "llm = ChatDartmouth(model_name=\"llama-3-1-8b-instruct\", seed=42, temperature=0.0)\n",
    "parser = JsonOutputParser()\n",
    "\n",
    "chain = llm | parser\n",
    "\n",
    "response = chain.invoke(\n",
    "    \"Generate 30 different words that are well-suited to showcase how word embeddings work. \"\n",
    "    \"Draw the words from domains like animals, finance, and food. The food one should contain tomato \"\n",
    "    \"Return the words in JSON format, using the domain as the key, and the words as values. \"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = pd.DataFrame.from_dict(response).melt(var_name=\"domain\", value_name=\"word\")\n",
    "\n",
    "embeddings = DartmouthEmbeddings(model_name=\"bge-large-en-v1-5\")\n",
    "words[\"embedding\"] = embeddings.embed_documents(words[\"word\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "**Note:** It is difficult to visualize a 1024 dimensional vector, as we're not 1024 dimensional humans! One way to get around this is by using a [UMAP](https://umap-learn.readthedocs.io/en/latest/) (Uniform Manifold Approximation and Projection) to represent this large vector as a 2 dimesional one. \n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "\n",
    "embeddings_list = words[\"embedding\"].to_list()\n",
    "mapper = umap.UMAP().fit(embeddings_list)\n",
    "umap_embeddings = pd.DataFrame(mapper.transform(embeddings_list), columns=[\"UMAP_x\", \"UMAP_y\"])\n",
    "\n",
    "words = pd.concat([words, umap_embeddings], axis=1)\n",
    "\n",
    "words.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a scatter plot using matplotlib\n",
    "for i in words[\"domain\"].unique():\n",
    "    subset = words[words[\"domain\"] == i]\n",
    "    plt.scatter(subset[\"UMAP_x\"], subset[\"UMAP_y\"], label=i)\n",
    "    \n",
    "    # Add the text labels\n",
    "    for j in range(len(subset)):\n",
    "        plt.text(\n",
    "            subset[\"UMAP_x\"].iloc[j],\n",
    "            subset[\"UMAP_y\"].iloc[j] + 0.12,\n",
    "            subset[\"word\"].iloc[j],\n",
    "            horizontalalignment=\"center\",\n",
    "            verticalalignment=\"center\",\n",
    "            fontsize=6,\n",
    "        )\n",
    "\n",
    "# Add legend and show plot\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.title(\"UMAP Projection of Words\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that groups with words related to foods, and animals, and finance are somewhat close to each other. This let's us visualize the similarities we saw before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "This recipe showed how to find the similarity between two embeddings. Visualizing embeddings can be a good way to represent this similarity. UMAP can be used to represent high-dimensional embeddings in a 2D plane, so we can easily visualize embeddings, and see their similarities."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
