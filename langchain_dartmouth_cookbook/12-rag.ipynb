{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation (RAG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous two recipes, we learned how to [obtain an embedding](./10-embeddings.ipynb), as well as how to express the [similarity](./11-similarity-search.ipynb) between two embeddings. Retrieval Augmented Generation (RAG) uses both of these techniques to provide relevant context to an LLM at query-time to ground the LLM's output in a knowledge base. For example, if you want the LLM to answer questions based on specific documents you have, you can use these documents as your knowledge base and implement a RAG pipeline.\n",
    "\n",
    "A typical RAG pipeline consists of three components: a vectorstore to hold the document embeddings, a retriever to retrieve relevant documents from the vectorstore based on a query, and an LLM to generate a response based on the query and the retrieved documents.\n",
    "\n",
    "This recipe will implement such a pipeline using componets from `langchain_dartmouth`, as well as the larger LangChain ecosystem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Manual RAG pipeline\n",
    "\n",
    "If we know which relevant context we want to provide, we could simply use string manipulation to add the context to the query. For example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_dartmouth.llms import ChatDartmouth\n",
    "\n",
    "llm = ChatDartmouth(model_name=\"llama-3-1-8b-instruct\")\n",
    "\n",
    "# User's question\n",
    "query = \"Are asteroids going to hit me?\"\n",
    "\n",
    "# Context relevant to the question from our knowledge base\n",
    "relevant_document = \"Asteroids do not generally hit people. There is a very low chance for that to happen\"\n",
    "\n",
    "# Augment prompt\n",
    "augmented_prompt = (\n",
    "    relevant_document + \"Considering this, answer the following question:\" + query\n",
    ")\n",
    "\n",
    "# Generate the answer\n",
    "response = llm.invoke(augmented_prompt)\n",
    "response.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a Vector Store\n",
    "\n",
    "That's great, but how can we find the relevant document in a collection of documents? That is where [similarity search](./11-similarity-search.ipynb) can help us:\n",
    "\n",
    "We can calculate the similarity between our user's query and all documents in our collection. Using similarity as a proxy for relevance, we can then retrieve, for example, the top 5 documents and use them as context.\n",
    "\n",
    "While we could write our own loop to go through the collection of embedded documents, there are optimized structures for storing embeddings and doing these kinds of operations on them called _vector stores_.\n",
    "\n",
    "```{hint}\n",
    "There are many different implementations of vector stores available, most of which have a corresponding LangChain class. Each implementation may have particular advantages and disadvantages, and the choice of vector store should be made based on your project's requirements. \n",
    "\n",
    "In this recipe, we will be using an [in-memory vector store](https://python.langchain.com/api_reference/core/vectorstores/langchain_core.vectorstores.in_memory.InMemoryVectorStore.html#inmemoryvectorstore). This vector store is a good choice to demonstrate the involved concepts, but it would be a very poor choice for a real-world project. Popular options for vector stores are [ChromaDB](https://python.langchain.com/api_reference/community/vectorstores/langchain_community.vectorstores.chroma.Chroma.html#chroma), [PGVector](https://python.langchain.com/api_reference/community/vectorstores/langchain_community.vectorstores.pgvector.PGVector.html#pgvector), or commerical offerings like [Pinecone](https://python.langchain.com/api_reference/pinecone/vectorstores/langchain_pinecone.vectorstores.PineconeVectorStore.html#pineconevectorstore).\n",
    "```\n",
    "\n",
    "Let's build a vector store for a collection of documents on various (very different) topics:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "print([p.name for p in Path(\"./rag_documents/\").glob(\"*.txt\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the vector store strings together quite a few components. Most of these have been introduced in the previous two recipes on [embeddings](./10-embeddings.ipynb) and [similarity search](./11-similarity-search.ipynb):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "from langchain_dartmouth.embeddings import DartmouthEmbeddings\n",
    "\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "\n",
    "\n",
    "# Load all files in a directory using the TextLoader class\n",
    "loader = DirectoryLoader(\"./rag_documents\", glob=\"**/*.txt\", loader_cls=TextLoader)\n",
    "collection = loader.load()\n",
    "\n",
    "# Initialize the text splitter with appropriate chunk size\n",
    "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    encoding_name=\"cl100k_base\", chunk_size=256, chunk_overlap=0\n",
    ")\n",
    "\n",
    "# Load and split the files\n",
    "documents = loader.load_and_split(text_splitter=text_splitter)\n",
    "\n",
    "embeddings_model = DartmouthEmbeddings(model_name=\"bge-large-en-v1-5\")\n",
    "\n",
    "# Initialize vector store and add documents\n",
    "vector_store = InMemoryVectorStore(embedding=embeddings_model)\n",
    "_ = vector_store.add_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{hint}\n",
    "\n",
    "The `DirectoryLoader` function is a class from LangChain that accepts a directory, a regex expression, and a [loader class](./10-embeddings.ipynb). It's a convenient way to load several documents living in the same directory at once.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use the vector store's `similarity_search` method to find the most relevant documents (or document chunks) in the collection given our query. We can change the number of returned documents using the parameter `k`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What killed the dinosaurs?\"\n",
    "\n",
    "docs = vector_store.similarity_search(query, k=2)\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, the similarity search retrieved two chunks from the `asteroids.txt` file! Since our query was related to asteroids, that makes sense! \n",
    "\n",
    "\n",
    "```{note}\n",
    "Note that the retrieved documents (chunks of the original file) are specifically related to the query!\n",
    "```\n",
    "\n",
    "We can now augment our prompt with the retrieved documents, just like we did before in the manual RAG:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_prompt = (\n",
    "    \"Answer the following query: \"\n",
    "    + query\n",
    "    + \"\\n\\nBase your response on the following context: \\n\\n\"\n",
    ")\n",
    "for doc in docs:\n",
    "    augmented_prompt += doc.page_content + \"\\n--\\n\"\n",
    "\n",
    "response = llm.invoke(augmented_prompt)\n",
    "\n",
    "response.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And there it is: A fully automated RAG pipeline!\n",
    "\n",
    "```{hint}\n",
    "You could streamline this even further and automate the prompt augmentation using LangChain's prompt templates. While this is beyond the scope of this recipe, you can check out [LangChain's RAG tutorial](https://python.langchain.com/docs/tutorials/rag/#orchestration) to learn more!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reranking Documents\n",
    "\n",
    "Setting the right value for `k` can be challenging: Retrieving many documents (a large `k`) casts a wide net and helps to ensure we don't miss anything relevant in the collection, but it also injects a lot of less relevant information into the context, potentially confusing the model and increasing the token consumption. A small `k` keeps the context focused and the response time low, but may miss important bits. Also, we are using similarity as a proxy for relevance, which may not necessarily be accurate. \n",
    "\n",
    "To deal with this issue, the concept of _reranking_ is often applied: \n",
    "\n",
    "1. Retrieve a large number of potentially relevant documents from the vector store using semantic similarity\n",
    "2. Rerank the documents based on their contextual relevance\n",
    "3. Use only the top N documents for response generation\n",
    "\n",
    "`langchain_dartmouth` offers the class `DartmouthReranker`, which you can use to reduce (compress) the number of documents after the similarity search:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_dartmouth.retrievers.document_compressors import DartmouthReranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reranker = DartmouthReranker(model_name=\"bge-reranker-v2-m3\", top_n=3)\n",
    "\n",
    "docs = vector_store.similarity_search(query, k=10)\n",
    "\n",
    "ranked_docs = reranker.compress_documents(query=query, documents=docs)\n",
    "\n",
    "for doc in ranked_docs:\n",
    "    print(doc.metadata[\"source\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that when our query is related to asteroids, the reranker correctly ranks chunks from the file `asteroids.txt` as the most relevant documents!\n",
    "\n",
    "Just like with LLMs and embedding models, you can list the available reranking models using the static method `list()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DartmouthReranker.list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "In this recipe, we have learned how to use a vector store for similarity search on a collection of documents given a query. By retrieving the most similar documents, we can implement a Retrieval Augmented Generation pipeline to ground an LLM's responses in our document collection.\n",
    "\n",
    "Finally, we have seen that a reranking model can be used to compress the list of documents based on their contextural relevance, as opposed to their semantic similarity, to reduce the irrelevant information we are passing to the LLM."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
