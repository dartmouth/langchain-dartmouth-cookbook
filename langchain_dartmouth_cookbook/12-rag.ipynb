{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval Augemented Generation (RAG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the prevous two recepies, we learned how to [obtain an embedding](./10-embeddings.ipynb), as well as how to find the [similarity](./11-similarity-search.ipynb) between two embeddings. Retrievel Augmente Generation uses both of these to achieve the goal of querying an LLM and also providing some documents for additional context. For example, if you want the LLM to answer some questions based on specific documents you have, you can feed those documents in \n",
    "\n",
    "A RAG model is primarily made up of two components, the retriever, and the reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.docstore.document import Document\n",
    "from langchain_dartmouth.retrievers.document_compressors import DartmouthReranker\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.document_loaders import TextLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a **collection** of documents, we are interested in finding the ones most relevent to our question. Once a query is posed, the `compress_documents()` function from the `DartmouthReranker` class can be used, which uses the concept about [similarity](./11-similarity-search.ipynb), to return a list of documents which are most relevant to the query.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Simple RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At it's heart a RAG is akin to just pluging in an entire chunk of text that we want the LLM to reference when in it's answer. An example of this can be found below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_dartmouth.llms import ChatDartmouth\n",
    "llm = ChatDartmouth(model_name=\"llama-3-1-8b-instruct\")\n",
    "\n",
    "relevant_document = \"Asteroids do not generally hit people. There is a 1 in a 7 billion chance for that to happen\"\n",
    "query = \"Are asteroids going to hit me?\"\n",
    "\n",
    "response = llm.invoke(relevant_document + 'Considering this, answer the following question:' +  query)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reranking Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was simple! What if we have many documents? In that case we can use the `DartmouthReranker` to find which document is the most relevant. \n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "**Note:** The `DirectoryLoader` function is a part of LangChain that accepts a directory, a regex expression, and a [loader class](./10-embeddings.ipynb). It's an easy way to load several documents that are in a directory at once\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the documents\n",
    "loader = DirectoryLoader('./rag_documents', glob=\"**/*.txt\", loader_cls=TextLoader)\n",
    "collection = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reranker = DartmouthReranker()\n",
    "\n",
    "query = \"Are asteroids going to hit me?\"\n",
    "\n",
    "ranked_docs = reranker.compress_documents(query=query, documents=collection)\n",
    "\n",
    "for doc in ranked_docs:\n",
    "    print(doc.metadata['source'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that when our query is related to asteroids, the document correcly ranks the asteroids.txt as the most relevent document. Now, we can prepend the content of the first ranked document to our query to get a response which considers the information in the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_dartmouth.llms import ChatDartmouth\n",
    "\n",
    "llm = ChatDartmouth(model_name=\"llama-3-1-8b-instruct\")\n",
    "response = llm.invoke(ranked_docs[0].page_content + 'Considering this, answer the following question:' +  query)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Vector Stores\n",
    "There is a useful feature in Langchain called `vector stores`. This allows us to store several documents in one go. The best part of this is that we don't need to prepend an entire document. We can break the documents into smaller chunks and only use the most relevent parts. You can learn more about `vector stores` [here](https://python.langchain.com/docs/how_to/#vector-stores). This is an example of an easy way to `embed_documents`. By using the `CharacterTextSplitter`, we can define our **chunk_size**, which is to avoid the batch size issue we saw in [similarity-search]('../langchain_dartmouth_cookbook/11-similarity-search.ipynb'). We can now add these chunks into our vector store. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain_dartmouth.embeddings import DartmouthEmbeddings\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "# Initialize the text splitter with appropriate chunk size\n",
    "text_splitter = CharacterTextSplitter(separator=\" \", chunk_size=100, chunk_overlap=50,length_function=len)\n",
    "\n",
    "# Load and split the documents\n",
    "collection = loader.load()\n",
    "chunks = text_splitter.split_documents(collection)\n",
    "\n",
    "# Initialize vector store and add chunks\n",
    "vector_store = InMemoryVectorStore(embedding=DartmouthEmbeddings())\n",
    "vector_store.add_documents(chunks)\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now when we are searching for relevent parts of our questions, the `similarity_search` method of the vector_store will pinpoint the most relevant parts for our question. These can then be used as part of the prompt. \n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "**Note:** Below we can see that the `page_content` for each of the vector stores is relevent not only to asteroids, but the particular part of the document that discusses the chances of asteroids hitting someone\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = vector_store.similarity_search(query)\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get all the relevent content from the documents\n",
    "relevent_content = ' '.join(doc.page_content for doc in docs)\n",
    "response = llm.invoke(relevent_content + 'Considering this, answer the following question:' +  query)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "In this recipe we use a query and `DartmouthReranker` to retrieve relative documents from a **collection**. The content of the highest ranked document is then prepended in the a prompt to an llm. This is an implementation of retreival augmented generation. We can also use a `vector_store` to do this, and pin-point the exact sections of the docment we want to reference using `similarity_search`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
