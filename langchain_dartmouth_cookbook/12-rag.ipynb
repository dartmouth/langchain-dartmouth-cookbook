{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval Augemented Generation (RAG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the prevous two recepies, we learned how to [obtain an embedding](./10-embeddings.ipynb), as well as how to find the [similarity](./11-similarity-search.ipynb) between two embeddings. Retrievel Augmente Generation uses both of these to achieve the goal of querying an LLM and also providing some documents for additional context. For example, if you want the LLM to answer some questions based on specific documents you have, you can feed those documents in \n",
    "\n",
    "A RAG model is primarily made up of two components, the retriever, and the reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.docstore.document import Document\n",
    "from langchain_dartmouth.retrievers.document_compressors import DartmouthReranker\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.document_loaders import TextLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a **collection** of documents, we are interested in finding the ones most relevent to our question. Once a query is posed, the `compress_documents()` function from the `DartmouthReranker` class can be used, which uses the concept about [similarity](./11-similarity-search.ipynb), to return a list of documents which are most relevant to the query.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Simple RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At it's heart a RAG is akin to just pluging in an entire chunk of text that we want the LLM to reference when in it's answer. An example of this can be found below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given the 1 in 7 billion chance of an asteroid hitting a person, the likelihood of an asteroid impacting you specifically is extremely low. \n",
      "\n",
      "To put it into perspective, you are more likely to win the lottery or be struck by lightning multiple times than be hit by an asteroid.\n"
     ]
    }
   ],
   "source": [
    "from langchain_dartmouth.llms import ChatDartmouth\n",
    "llm = ChatDartmouth(model_name=\"llama-3-1-8b-instruct\")\n",
    "\n",
    "relevant_document = \"Asteroids do not generally hit people. There is a 1 in a 7 billion chance for that to happen\"\n",
    "query = \"Are asteroids going to hit me?\"\n",
    "\n",
    "response = llm.invoke(relevant_document + 'Considering this, answer the following question:' +  query)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reranking Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was simple! What if we have many documents? In that case we can use the `DartmouthReranker` to find which document is the most relevant. \n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "**Note:** The `DirectoryLoader` function is a part of LangChain that accepts a directory, a regex expression, and a [loader class](./10-embeddings.ipynb). It's an easy way to load several documents that are in a directory at once\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the documents\n",
    "loader = DirectoryLoader('./rag_documents', glob=\"**/*.txt\", loader_cls=TextLoader)\n",
    "collection = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rag_documents\\asteroids.txt\n",
      "rag_documents\\hot_sauce.txt\n",
      "rag_documents\\history.txt\n"
     ]
    }
   ],
   "source": [
    "reranker = DartmouthReranker()\n",
    "\n",
    "query = \"Are asteroids going to hit me?\"\n",
    "\n",
    "ranked_docs = reranker.compress_documents(query=query, documents=collection)\n",
    "\n",
    "for doc in ranked_docs:\n",
    "    print(doc.metadata['source'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that when our query is related to asteroids, the document correcly ranks the asteroids.txt as the most relevent document. Now, we can prepend the content of the first ranked document to our query to get a response which considers the information in the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text states that \"Asteroids are very likely to not hit you. The chance is 1 in 1 billion.\" This is a fake estimate made for demonstration purposes, implying that asteroids are extremely unlikely to hit you.\n"
     ]
    }
   ],
   "source": [
    "from langchain_dartmouth.llms import ChatDartmouth\n",
    "\n",
    "llm = ChatDartmouth(model_name=\"llama-3-1-8b-instruct\")\n",
    "response = llm.invoke(ranked_docs[0].page_content + 'Considering this, answer the following question:' +  query)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Vector Stores\n",
    "There is a useful feature in Langchain called `vector stores`. This allows us to store several documents in one go. The best part of this is that we don't need to prepend an entire document. We can break the documents into smaller chunks and only use the most relevent parts. You can learn more about `vector stores` [here](https://python.langchain.com/docs/how_to/#vector-stores). This is an example of an easy way to `embed_documents`. By using the `CharacterTextSplitter`, we can define our **chunk_size**, which is to avoid the batch size issue we saw in [similarity-search]('../langchain_dartmouth_cookbook/11-similarity-search.ipynb'). We can now add these chunks into our vector store. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain_dartmouth.embeddings import DartmouthEmbeddings\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "# Initialize the text splitter with appropriate chunk size\n",
    "text_splitter = CharacterTextSplitter(separator=\" \", chunk_size=100, chunk_overlap=50,length_function=len)\n",
    "\n",
    "# Load and split the documents\n",
    "collection = loader.load()\n",
    "chunks = text_splitter.split_documents(collection)\n",
    "\n",
    "# Initialize vector store and add chunks\n",
    "vector_store = InMemoryVectorStore(embedding=DartmouthEmbeddings())\n",
    "vector_store.add_documents(chunks)\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now when we are searching for relevent parts of our questions, the `similarity_search` method of the vector_store will pinpoint the most relevant parts for our question. These can then be used as part of the prompt. \n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "**Note:** Below we can see that the `page_content` for each of the vector stores is relevent not only to asteroids, but the particular part of the document that discusses the chances of asteroids hitting someone\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='49e6ea98-31d0-4acf-beeb-edecd87d8b0d', metadata={'source': 'rag_documents\\\\asteroids.txt'}, page_content='asteroids, marking a new era in space exploration. \\n\\nAsteroids are very likely to not hit you. The'),\n",
       " Document(id='ba86aba0-8e59-49a9-bf69-e65c621b5336', metadata={'source': 'rag_documents\\\\asteroids.txt'}, page_content='Asteroids are very likely to not hit you. The chance is 1 in 1 billion. This is a fake estimate'),\n",
       " Document(id='b2d699a0-ea3b-461e-a4bd-ab4394948a13', metadata={'source': 'rag_documents\\\\asteroids.txt'}, page_content='Asteroids**\\n\\nAsteroids are typically small, with diameters ranging from a few meters to hundreds of'),\n",
       " Document(id='878675eb-d58c-4e31-a681-3ed986dce468', metadata={'source': 'rag_documents\\\\asteroids.txt'}, page_content='implications. By studying asteroids, scientists can gain insights into the formation and evolution')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = vector_store.similarity_search(query)\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The chances of an asteroid hitting you are extremely low. While the exact odds are difficult to estimate, it's generally agreed that the likelihood of being hit by an asteroid is tiny.\n",
      "\n",
      "To put things into perspective, the estimate you mentioned earlier (1 in 1 billion) is likely an exaggeration. However, even if we consider more realistic estimates, the risk is still extremely low.\n",
      "\n",
      "For example, NASA estimates that the chances of being hit by a large asteroid (over 100 meters in diameter) are about 1 in 1 million every year. The chances of being hit by a smaller asteroid are even lower.\n",
      "\n",
      "To give you a better idea, here are some estimates of the annual risk of being hit by an asteroid of different sizes, based on NASA's data:\n",
      "\n",
      "* Small asteroid (less than 10 meters in diameter): 1 in 100 million\n",
      "* Medium asteroid (10-100 meters in diameter): 1 in 1 million\n",
      "* Large asteroid (over 100 meters in diameter): 1 in 1 billion (or even less)\n",
      "\n",
      "So, to answer your question, the chances of an asteroid hitting you are extremely low. You don't need to worry about it, but it's always interesting to learn about asteroids and the importance of asteroid detection and tracking programs.\n"
     ]
    }
   ],
   "source": [
    "# Let's get all the relevent content from the documents\n",
    "relevent_content = ' '.join(doc.page_content for doc in docs)\n",
    "response = llm.invoke(relevent_content + 'Considering this, answer the following question:' +  query)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "In this recipe we use a query and `DartmouthReranker` to retrieve relative documents from a **collection**. The content of the highest ranked document is then prepended in the a prompt to an llm. This is an implementation of retreival augmented generation. We can also use a `vector_store` to do this, and pin-point the exact sections of the docment we want to reference using `similarity_search`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
