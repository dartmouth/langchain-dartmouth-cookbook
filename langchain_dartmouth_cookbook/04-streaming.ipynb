{
   "cells": [
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "# Streaming LLM output\n",
            "\n",
            "Large Language Models produce output token by token, and each token can take some time to generate. If you are waiting for the model to finish the entire response before outputting the result, these times add up quickly!\n",
            "\n",
            "An alternative to this is streaming: Similar to video streaming, where you don't wait for the entire video to be downloaded before playing it, you can stream the output of an LLM. In this recipe, we will explore how to do that with `langchain_dartmouth`!\n",
            "\n",
            "```{note}\n",
            "\n",
            "Many LLMs in the LangChain ecosystem support streaming, not just the ones in `langchain_dartmouth`! You could replace the model in this notebook with, e.g., `ChatOpenAI` from `langchain_openai`, and it would work exactly the same!\n",
            "```"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Importing and instantiating a model\n",
            "\n",
            "Just as we saw in [the previous recipe](03-llms.ipynb), we will import a chat model and then instantiate it. We will use the `streaming` parameter, however, to tell the model that we want it to stream its output!"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 2,
         "metadata": {},
         "outputs": [],
         "source": [
            "from langchain_dartmouth.llms import ChatDartmouth\n",
            "from dotenv import find_dotenv, load_dotenv\n",
            "\n",
            "load_dotenv(find_dotenv())"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 3,
         "metadata": {},
         "outputs": [],
         "source": [
            "llm = ChatDartmouth(model_name=\"llama-3-1-8b-instruct\", streaming=True)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Streaming the output\n",
            "\n",
            "We could use the `invoke` method as we did before, but that would still require us to wait for the entire response to be generated before returning. Since we have set our model's `streaming` parameter to `True`, we can instead call the `stream` method. This will return a generator object. We can then iterate through this generator and print each returned chunk as it is generated:"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "for chunk in llm.stream(\"Write a haiku about Dartmouth College\"):\n",
            "    print(chunk.content)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "We can see that the chunks came in one by one, because the `print` function breaks the line after every chunk. We can use the `end` parameter to avoid that:"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "for chunk in llm.stream(\"Write a haiku about Dartmouth College\"):\n",
            "    print(chunk.content, end=\"\")"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "That looks better! Let's try a longer response to show the benefit of streaming:"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "for chunk in llm.stream(\"Write five haiku about Dartmouth College\"):\n",
            "    print(chunk.content, end=\"\")"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Summary\n",
            "\n",
            "This recipe showed how to stream output from an LLM using the `stream` method. Streaming long responses makes for a better user experience and a more efficient use of time when working with an LLM interactively."
         ]
      }
   ],
   "metadata": {
      "kernelspec": {
         "display_name": ".venv",
         "language": "python",
         "name": "python3"
      },
      "language_info": {
         "codemirror_mode": {
            "name": "ipython",
            "version": 3
         },
         "file_extension": ".py",
         "mimetype": "text/x-python",
         "name": "python",
         "nbconvert_exporter": "python",
         "pygments_lexer": "ipython3",
         "version": "3.12.6"
      }
   },
   "nbformat": 4,
   "nbformat_minor": 2
}