{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d21bfc3d",
   "metadata": {},
   "source": [
    "# Tool Calling\n",
    "\n",
    "Large Language Models are powerful text processors, but they have inherent limitations: They can't perform precise calculations, access real-time information, or interact with external systems directly. **Tool calling** solves this by allowing LLMs to invoke Python functions that extend their capabilities beyond text generation.\n",
    "\n",
    "With tool calling, you can build applications where LLMs can:\n",
    "- Perform accurate mathematical calculations\n",
    "- Retrieve information from databases or APIs\n",
    "- Interact with external services (weather, search engines, etc.)\n",
    "- Execute custom business logic\n",
    "- Access and process real-time data\n",
    "  \n",
    "\n",
    "\n",
    "## How Tool Calling Works\n",
    "\n",
    "The tool calling workflow creates a bridge between the LLM and your Python functions:\n",
    "\n",
    "1. You define Python functions and make them available as tools\n",
    "2. The LLM analyzes user queries and decides when to use these tools\n",
    "3. The LLM generates appropriate function calls with parameters\n",
    "4. Your code executes the functions and returns results\n",
    "5. The LLM incorporates these results into its final response\n",
    "\n",
    "This pattern enables LLMs to go beyond their training data and perform actions in the real world.\n",
    "\n",
    "```{hint}\n",
    "Tool calling is the foundation for building **agents** (covered in the [next recipe](14-agents.ipynb)), which can use multiple tools iteratively to accomplish complex, multi-step tasks.\n",
    "```\n",
    "\n",
    "Let's start by defining our first tool!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e83c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import find_dotenv, load_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9836e3c",
   "metadata": {},
   "source": [
    "## What Are Tools?\n",
    "\n",
    "In LangChain, a [**tool**](https://docs.langchain.com/oss/python/langchain/tools) is a Python function that an LLM can invoke. When you make a function available as a tool, the LLM receives a schema describing the function's structure, which it uses to decide when and how to call it.\n",
    "\n",
    "Each tool consists of four key components:\n",
    "\n",
    "- **Name**: An identifier that the model uses to reference the tool (typically derived from the function name)\n",
    "- **Description**: A natural language explanation that guides the LLM on when and how to use the tool\n",
    "- **Parameters**: The function's arguments, including their types and purposes\n",
    "- **Return type**: The type of value the function returns\n",
    "\n",
    "\n",
    "For tools to work effectively, LLMs need clear information about what each tool does and how to use it. This is where Python's documentation features become critical:\n",
    "\n",
    "**Docstrings**: The first line of your function's docstring becomes the tool's description. This is what the LLM reads to understand when to use your tool. Clear, specific descriptions help the model make better decisions.\n",
    "\n",
    "**Type hints**: Type annotations on parameters and return values define the tool's schema. They tell the LLM what kind of arguments to provide and what to expect back. Without type hints, the LLM may not know how to properly invoke your tool.\n",
    "\n",
    "```{note}\n",
    "Think of docstrings and type hints as the \"instructions\" you're giving to the LLM. The more precise and descriptive they are, the better the LLM will be at using your tools correctly!\n",
    "```\n",
    "\n",
    "Let's see how to create a tool in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d09ec5",
   "metadata": {},
   "source": [
    "## Defining Your First Tool\n",
    "\n",
    "LangChain provides the [`@tool`](https://reference.langchain.com/python/langchain/tools/?h=tool) decorator to convert ordinary Python functions into tools that LLMs can invoke. Let's create a simple multiplication tool:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162e6bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "\n",
    "@tool\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply two numbers.\n",
    "\n",
    "    Always use this tool when trying to multiply numbers.\n",
    "    \"\"\"\n",
    "    return a * b\n",
    "\n",
    "\n",
    "# The decorator turns the function into a Tool object\n",
    "print(f\"Type: {type(multiply)}\")\n",
    "print(f\"Tool name: {multiply.name}\")\n",
    "print(f\"Tool description: {multiply.description}\")\n",
    "print(f\"Tool parameters: {multiply.args}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df0f480",
   "metadata": {},
   "source": [
    "When we apply the `@tool` decorator, LangChain automatically extracts metadata from our function:\n",
    "\n",
    "- The **function name** (`multiply`) becomes the tool's identifier\n",
    "- The **docstring** (`\"Multiply two numbers.\"`) becomes the description the LLM sees\n",
    "- The **type hints** (`a: int, b: int -> int`) define the parameter schema\n",
    "\n",
    "The `args` property shows the JSON schema that describes the tool's parameters to the LLM. This schema is what enables the model to generate valid function calls with the correct argument types.\n",
    "\n",
    "```{hint}\n",
    "Keep your tool descriptions concise but specific. You can include information on the appropriate time to use this tool as part of this description!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3fd2be",
   "metadata": {},
   "source": [
    "## Binding Tools to Models\n",
    "\n",
    "Creating a tool is only the first step. To make it available to an LLM, we need to **bind** it to the model using the [`bind_tools()`](https://reference.langchain.com/python/langchain_core/language_models/?h=#langchain_core.language_models.BaseChatModel.bind_tools) method. This will ensure that the tool's schema (name, description, and parameter definitions) is send to to the model as part of any invocation:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36bc3692",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_dartmouth.llms import ChatDartmouth\n",
    "\n",
    "model = ChatDartmouth(model_name=\"openai.gpt-oss-120b\", temperature=0.0)\n",
    "tools = [multiply]\n",
    "model_with_tools = model.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3edd71f6",
   "metadata": {},
   "source": [
    "Now the model is aware of the `multiply` tool and its capabilities. When you send a message to `model_with_tools`, the LLM can choose to:\n",
    "\n",
    "1. **Respond directly** with text if it can answer without tools\n",
    "2. **Call the tool** if it needs to perform a calculation or action\n",
    "\n",
    "The key insight is that the model **decides** when to use tools based on the user's query and the tool descriptions. The LLM won't actually execute the tool-that's still your responsibility. Tool binding simply makes the model aware that these functions exist and how to call them.\n",
    "\n",
    "```{note}\n",
    "Not all models support tool calling. You can check which models have this capability by running [`ChatDartmouth.list()`](03-llms.ipynb) and looking for `\"tool_calling\"` in the `capabilities` list for each model.\n",
    "```\n",
    "\n",
    "Let's see the complete workflow in action!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9cd3bb2",
   "metadata": {},
   "source": [
    "## The Tool Calling Workflow\n",
    "\n",
    "Now let's walk through the complete tool calling process step-by-step. The workflow involves a conversation between the user, the model, and the tool:\n",
    "\n",
    "1. **User sends a message** requiring tool use\n",
    "2. **Model analyzes the query** and decides to invoke a tool\n",
    "3. **Extract the tool call** from the model's response\n",
    "4. **Execute the tool** with the provided arguments\n",
    "5. **Send the tool result** back to the model as a new message\n",
    "6. **Model synthesizes** a final natural language response\n",
    "\n",
    "Let's implement this workflow:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db77361d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# Step 1: Create initial message\n",
    "messages = [HumanMessage(\"What is 5 times 3?\")]\n",
    "\n",
    "# Step 2: Model responds with tool call\n",
    "response = model_with_tools.invoke(messages)\n",
    "messages.append(response)\n",
    "\n",
    "# Step 3 & 4: Execute tool if called\n",
    "if tool_calls := response.tool_calls:\n",
    "    for tool_call in tool_calls:\n",
    "        # Find the matching tool\n",
    "        fn, *_ = [tool for tool in tools if tool.name == tool_call[\"name\"]]\n",
    "        # Execute it and create tool message\n",
    "        tool_msg = fn.invoke(tool_call)\n",
    "        messages.append(tool_msg)\n",
    "\n",
    "# Step 5 & 6: Get final response\n",
    "final_response = model_with_tools.invoke(messages)\n",
    "messages.append(final_response)\n",
    "\n",
    "# Display the conversation\n",
    "for message in messages:\n",
    "    message.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c59f7c",
   "metadata": {},
   "source": [
    "Let's examine the conversation flow that just occurred:\n",
    "\n",
    "- **Human Message**: The user's question (`\"What is 5 times 3?\"`)\n",
    "- **AI Message** (first): Contains the tool call with arguments `a=5, b=3`—the model decided to use the `multiply` tool\n",
    "- **Tool Message**: The result of executing `multiply(5, 3)`, which is `15`\n",
    "- **AI Message** (final): The natural language response incorporating the tool result\n",
    "\n",
    "Notice that the model doesn't just return the raw number \"15\". Instead, it contextualizes the result in natural language.\n",
    "\n",
    "```{hint}\n",
    "The `tool_calls` attribute on the AI message contains a list of tool invocations. A model can request multiple tool calls in a single response! We use the walrus operator (`:=`) to both check if the list exists and assign it to a variable in one line.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d75ae1b",
   "metadata": {},
   "source": [
    "## Working with Multiple Tools\n",
    "\n",
    "When you bind multiple tools to a model, the LLM analyzes the user's query and selects the most appropriate tool based on the tool descriptions. This allows you to build systems with specialized capabilities for different tasks.\n",
    "\n",
    "Let's create a calculator with multiple operations:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da691646",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Add two numbers together. Always use this tool when trying to add numbers.\"\"\"\n",
    "    return a + b\n",
    "\n",
    "\n",
    "@tool\n",
    "def subtract(a: int, b: int) -> int:\n",
    "    \"\"\"Subtract the second number from the first. Always use this tool when trying to subtract numbers.\"\"\"\n",
    "    return a - b\n",
    "\n",
    "\n",
    "@tool\n",
    "def divide(a: float, b: float) -> float:\n",
    "    \"\"\"Divide the first number by the second. Always use this tool when trying to divide numbers.\"\"\"\n",
    "    if b == 0:\n",
    "        return \"Error: Division by zero\"\n",
    "    return a / b\n",
    "\n",
    "\n",
    "# Bind all tools\n",
    "tools = [add, subtract, multiply, divide]\n",
    "model_with_tools = model.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b7b85b",
   "metadata": {},
   "source": [
    "Now let's test the model's ability to select the correct tool for different queries:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f283be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with various math questions\n",
    "test_queries = [\n",
    "    \"What is 35 plus 27?\",\n",
    "    \"Calculate 100 divided by 4\",\n",
    "    \"What is 50 minus 18?\",\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\nQuery: {query}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    messages = [HumanMessage(query)]\n",
    "    response = model_with_tools.invoke(messages)\n",
    "    messages.append(response)\n",
    "\n",
    "    if tool_calls := response.tool_calls:\n",
    "        for tool_call in tool_calls:\n",
    "            print(f\"Tool called: {tool_call['name']}\")\n",
    "            print(f\"Arguments: {tool_call['args']}\")\n",
    "\n",
    "            fn, *_ = [tool for tool in tools if tool.name == tool_call[\"name\"]]\n",
    "            tool_msg = fn.invoke(tool_call)\n",
    "            messages.append(tool_msg)\n",
    "\n",
    "    final_response = model_with_tools.invoke(messages)\n",
    "    messages.append(final_response)\n",
    "    print(f\"Final answer: {final_response.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ce654e",
   "metadata": {},
   "source": [
    "The model successfully chooses the correct tool for each query:\n",
    "- \"plus\" triggers the `add` tool\n",
    "- \"divided by\" triggers the `divide` tool  \n",
    "- \"minus\" triggers the `subtract` tool\n",
    "\n",
    "The model's tool selection is based on semantic understanding of the query combined with the tool descriptions. Clear, descriptive tool names and docstrings help the model make accurate choices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f2894c",
   "metadata": {},
   "source": [
    "## Connecting the Concepts\n",
    "\n",
    "Tool calling is a foundational pattern that connects to several other concepts in this cookbook:\n",
    "\n",
    "[**Agents**](14-agents.ipynb): Agents use tools iteratively to accomplish complex, multi-step tasks. While this recipe shows single tool invocations, agents can chain multiple tool calls together to solve problems that require reasoning and planning.\n",
    "\n",
    "[**Chains**](08-building-chains.ipynb): Tools can be incorporated into LangChain chains, allowing you to build sophisticated pipelines that combine LLM reasoning with external computations and data retrieval.\n",
    "\n",
    "[**Structured Output**](15-structured-output.ipynb): Both tool calling and structured output use schemas to constrain LLM outputs. Tool calling focuses on function invocation, while structured output focuses on data formatting.\n",
    "\n",
    "In the next recipe, we'll see how agents build on tool calling to create autonomous systems that can use multiple tools strategically to accomplish user goals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90207bb5",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this recipe, we learned how to extend LLM capabilities through tool calling. \n",
    "\n",
    "- **Tool calling** allows LLMs to invoke Python functions\n",
    "- The **`@tool` decorator** converts Python functions into LangChain tools by extracting metadata from function signatures and docstrings\n",
    "- **`bind_tools()`** makes tools available to a model by sending their schemas (names, descriptions, parameters) with every message\n",
    "- The **tool calling workflow** involves: user query → model decides to call tool → tool execution → model synthesizes response\n",
    "- **Clear docstrings and type hints** are essential because they guide the LLM's understanding of when and how to use tools\n",
    "- **Multiple tools** enable specialized capabilities, with the model selecting appropriate tools based on semantic understanding\n",
    "- **Error handling** should return descriptive messages rather than raising exceptions\n",
    "\n",
    "Tool calling is the foundation for building agents and other advanced LLM applications. In the next recipe, we'll explore how agents use tools iteratively to accomplish complex tasks.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
