{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import find_dotenv, load_dotenv\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity\n",
    "\n",
    "There are several benefits to having the embedding of a word, a primary one is that it gives us the ability to compare how close two words are in meaning. One way of simpling doing so is by taking the **normalized dot product**.\n",
    "\n",
    "The similarity between two embeddings is given by their dot product. We then normalize the dot product with the size of each of the vectors. That way if one word is really common, it won't skew the similarity. \n",
    "\n",
    "$$\n",
    "\\text{Similarity} = \\frac{v\\cdot w}{|v||w|}\n",
    "$$\n",
    "\n",
    "First let embed two words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_dartmouth.embeddings import DartmouthEmbeddings\n",
    "\n",
    "embeddings = DartmouthEmbeddings()\n",
    "text_1 = \"Japan\"\n",
    "text_2 = \"Pizza\"\n",
    "\n",
    "embed_1 = embeddings.embed_query(text_1)\n",
    "embed_2 = embeddings.embed_query(text_2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's do the dot product\n",
    "$$\n",
    "v\\cdot w = \\sum_{i = 1}^N(v_i \\times w_i)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "multiplied_list = []\n",
    "for i in range(length_of_vector):\n",
    "    multiplied_list.append(embed_1[i] * embed_2[i])\n",
    "\n",
    "dot_product = sum(multiplied_list)\n",
    "print(f'The dot product of the two vectors is: {dot_product:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_1_size = []\n",
    "embed_2_size = []\n",
    "\n",
    "for j in range(len(embed_2)): \n",
    "    embed_1_size.append(embed_1[j] * embed_1[j])\n",
    "    embed_2_size.append(embed_2[j] * embed_2[j])\n",
    "\n",
    "sum_embed_1_size = sum(embed_1_size)**0.5\n",
    "sum_embed_2_size = sum(embed_2_size)**0.5\n",
    "\n",
    "normalized_size = sum_embed_1_size * sum_embed_2_size\n",
    "\n",
    "print(f'The normalized size of the two vectors is: {normalized_size:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting this together, we can obtain the similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity = dot_product / normalized_size\n",
    "print(f\"Similarity between '{text_1}' and '{text_2}': {similarity:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing an embedding\n",
    "A better way to understand an embedding is to visualize it. Let's generate some random words related to different domains, and find their embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatDartmouth(model_name=\"llama-3-1-8b-instruct\", seed=42, temperature=0.0)\n",
    "parser = JsonOutputParser()\n",
    "\n",
    "chain = llm | parser\n",
    "\n",
    "response = chain.invoke(\n",
    "    \"Generate 30 different words that are well-suited to showcase how word embeddings work. \"\n",
    "    \"Draw the words from domains like animals, finance, and food. The food one should contain tomato \"\n",
    "    \"Return the words in JSON format, using the domain as the key, and the words as values. \"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = pd.DataFrame.from_dict(response).melt(var_name=\"domain\", value_name=\"word\")\n",
    "\n",
    "embeddings = DartmouthEmbeddings(model_name=\"bge-large-en-v1-5\")\n",
    "words[\"embedding\"] = embeddings.embed_documents(words[\"word\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is difficult to visualize a 1024 dimensional vector, as we're not 1024 dimensional humans! One way to get around this is by using a [UMAP](https://umap-learn.readthedocs.io/en/latest/) (Uniform Manifold Approximation and Projection) to represent this large vector as a 2 dimesional one. This can then be plotted as follows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting using UMAP\n",
    "mapper = umap.UMAP().fit(np.array(words[\"embedding\"].to_list()))\n",
    "umap_embeddings = pd.DataFrame(mapper.transform(np.array(words[\"embedding\"].to_list())), columns=[\"UMAP_x\", \"UMAP_y\"])\n",
    "\n",
    "# merge with the words\n",
    "words = pd.concat([words, umap_embeddings], axis=1)\n",
    "words.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a scatter plot using matplotlib\n",
    "for i in words[\"domain\"].unique():\n",
    "    subset = words[words[\"domain\"] == i]\n",
    "    plt.scatter(subset[\"UMAP_x\"], subset[\"UMAP_y\"], label=i)\n",
    "    \n",
    "    # Add the text labels\n",
    "    for j in range(len(subset)):\n",
    "        plt.text(\n",
    "            subset[\"UMAP_x\"].iloc[j],\n",
    "            subset[\"UMAP_y\"].iloc[j] + 0.12,\n",
    "            subset[\"word\"].iloc[j],\n",
    "            horizontalalignment=\"center\",\n",
    "            verticalalignment=\"center\",\n",
    "            fontsize=6,\n",
    "        )\n",
    "\n",
    "# Add legend and show plot\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.title(\"UMAP Projection of Words\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that groups with words related to foods, and animals, and finance are somewhat close to each other. This let's us find the similarity between different words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embed \n",
    "`embed_documents` lets us take advantage of langchain's  [document loaders](https://python.langchain.com/docs/integrations/document_loaders/) which let us read various formats of data into a document class, which is embeddable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.schema import Document\n",
    "\n",
    "file_path = './rag_documents/asteroids.txt'\n",
    "\n",
    "text_loader = TextLoader(file_path, encoding='utf-8')\n",
    "loaded_documents = text_loader.load()\n",
    "document = loaded_documents[0]\n",
    "\n",
    "words = document.page_content.split()\n",
    "print('Number of words in the document: ', len(words))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
