{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import find_dotenv, load_dotenv\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity\n",
    "\n",
    "There are several benefits to having the embedding of a word, a primary one is that it gives us the ability to compare how close two words are in meaning. One way of simpling doing so is by taking the **normalized dot product**.\n",
    "\n",
    "The similarity between two embeddings is given by their dot product. \n",
    "\n",
    "$$\n",
    "\\text{Similarity} = \\vec{v} \\cdot \\vec{w}\n",
    "$$\n",
    "\n",
    "First let embed two words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_dartmouth.embeddings import DartmouthEmbeddings\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "embeddings = DartmouthEmbeddings()\n",
    "text_1 = \"Japan\"\n",
    "text_2 = \"Pizza\"\n",
    "\n",
    "embed_1 = embeddings.embed_query(text_1)\n",
    "embed_2 = embeddings.embed_query(text_2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's do the dot product\n",
    "$$\n",
    "v\\cdot w = \\sum_{i = 1}^N(v_i \\cdot w_i)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "multiplied_list = []\n",
    "length_of_vector = len(embed_1)\n",
    "\n",
    "for i in range(length_of_vector):\n",
    "    multiplied_list.append(embed_1[i] * embed_2[i])\n",
    "\n",
    "dot_product = sum(multiplied_list)\n",
    "print(f\"Similarity between '{text_1}' and '{text_2}': {dot_product:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing an embedding\n",
    "A better way to understand an embedding is to visualize it. Let's generate some random words related to different domains, and find their embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import umap\n",
    "from langchain_dartmouth.llms import ChatDartmouth\n",
    "\n",
    "\n",
    "llm = ChatDartmouth(model_name=\"llama-3-1-8b-instruct\", seed=42, temperature=0.0)\n",
    "parser = JsonOutputParser()\n",
    "\n",
    "chain = llm | parser\n",
    "\n",
    "response = chain.invoke(\n",
    "    \"Generate 30 different words that are well-suited to showcase how word embeddings work. \"\n",
    "    \"Draw the words from domains like animals, finance, and food. The food one should contain tomato \"\n",
    "    \"Return the words in JSON format, using the domain as the key, and the words as values. \"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = pd.DataFrame.from_dict(response).melt(var_name=\"domain\", value_name=\"word\")\n",
    "\n",
    "embeddings = DartmouthEmbeddings(model_name=\"bge-large-en-v1-5\")\n",
    "words[\"embedding\"] = embeddings.embed_documents(words[\"word\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "**Note:** It is difficult to visualize a 1024 dimensional vector, as we're not 1024 dimensional humans! One way to get around this is by using a [UMAP](https://umap-learn.readthedocs.io/en/latest/) (Uniform Manifold Approximation and Projection) to represent this large vector as a 2 dimesional one. \n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "\n",
    "embeddings_list = words[\"embedding\"].to_list()\n",
    "mapper = umap.UMAP().fit(embeddings_list)\n",
    "umap_embeddings = pd.DataFrame(mapper.transform(embeddings_list), columns=[\"UMAP_x\", \"UMAP_y\"])\n",
    "\n",
    "words = pd.concat([words, umap_embeddings], axis=1)\n",
    "\n",
    "words.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a scatter plot using matplotlib\n",
    "for i in words[\"domain\"].unique():\n",
    "    subset = words[words[\"domain\"] == i]\n",
    "    plt.scatter(subset[\"UMAP_x\"], subset[\"UMAP_y\"], label=i)\n",
    "    \n",
    "    # Add the text labels\n",
    "    for j in range(len(subset)):\n",
    "        plt.text(\n",
    "            subset[\"UMAP_x\"].iloc[j],\n",
    "            subset[\"UMAP_y\"].iloc[j] + 0.12,\n",
    "            subset[\"word\"].iloc[j],\n",
    "            horizontalalignment=\"center\",\n",
    "            verticalalignment=\"center\",\n",
    "            fontsize=6,\n",
    "        )\n",
    "\n",
    "# Add legend and show plot\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.title(\"UMAP Projection of Words\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that groups with words related to foods, and animals, and finance are somewhat close to each other. This let's us find the similarity between different words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "This recipe showed how to find the similarity between two embeddings. Visualizing embeddings can be a good way to represent this similarity. UMAP can be used to represent high-dimensional embeddings in a 2D plane, so we can easily observe it. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
