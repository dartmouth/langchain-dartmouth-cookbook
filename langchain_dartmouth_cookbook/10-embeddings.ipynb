{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings\n",
    "\n",
    "A major challenge for Large Language Models (LLMs), and Natural Language Processing (NLP) in general, is modeling semantic context. Humans can very easily infer the meaning of a word in a particular context. Capturing this ability in machine-comprehensible way, however, is not a simple task, given that most algorithms (LLMs included) can only operate on _numbers_ using arithmetic operations, not on text.\n",
    "\n",
    "So before an LLM can \"understand\" and predict words, it first needs to convert them into numbers in a way that preserves the words' semantic context. This is done through a process called \"vector embedding\". The goal of the embedding is to find a numeric representation of a word (or longer piece of text) that yields similar numbers for semantically similar words. \n",
    "\n",
    "You can think of this as representing a string of text as a collection of sliders, where each slider setting captures some aspect of the word's meaning. For example, words like \"nice\" and \"stupendous\" might have similar settings on a \"positivity\" slider but differ on an \"intensity\" slider. We could now describe the settings of all sliders as a vector, where each element represents the setting of one slider. This vector is called the \"embedding\" of the word and its dimension (number of elements) is equal to the number of \"sliders\" representing the word.\n",
    "\n",
    "```{attention}\n",
    "\n",
    "A word's embedding involves many dimensions, possibly thousands. However, we don't actually know what each individual dimension represents in terms of semantic meaning. The high dimensionality better models complex relationships between words, even if we can't clearly label each one. So while the \"slider model\" is a good intuitive approach, it is not accurate in that sense and individual dimensions of embeddings should usually not be interpreted in a meaningful way. Only the relative similarity between vectors matters! Check out [the next recipe](11-similarity-search.ipynb) to learn more about that.\n",
    "```\n",
    "\n",
    "[This video](https://www.youtube.com/watch?v=wgfSDrqYMJ4) explains this concept in a little greater detail, if you're interested!\n",
    "\n",
    "When using an LLM, you usually don't need to generate the embeddings yourself. The first layer of an LLM, called the _embedding layer_, takes care of that for you. However, calculating embeddings is highly relevant in many tasks surrounding LLMs, like semantic search and Retrieval-Augmented Generation (RAG), which we will be building up to with this recipe and the two following ones on [similarity search](11-similarity-search.ipynb) and [RAG](12-rag.ipynb).\n",
    "\n",
    "This recipe will go over how to use an _embedding model_ provided by `langchain_dartmouth` to generate embeddings for text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import find_dotenv, load_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating an Embedding\n",
    "\n",
    "We create embeddings using an embedding model. Embedding models deployed on Dartmouth's compute resources are available through the `DartmouthEmbeddings` class.\n",
    "\n",
    "Its interface is different from the text-generation models used in the prevous recipes. The `embed_query` method takes in a string and returns the embedding of that string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_dartmouth.embeddings import DartmouthEmbeddings\n",
    "\n",
    "embeddings = DartmouthEmbeddings(model_name=\"bge-large-en-v1-5\")\n",
    "\n",
    "embedding_vector = embeddings.embed_query(\"tiger\")\n",
    "print(embedding_vector[:24], \"...\")\n",
    "print(\"Length of embedding: \", len(embedding_vector))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{note}\n",
    "\n",
    "We see that the word \"tiger\" is represented by a list of 1024 numbers. This means that the numeric representation of the word \"tiger\" consists of 1024 dimensions (or sliders) for this particular embedding model `bge-large-en-v1-5`. Other models may use fewer or more numbers to represent text. You can read more about the model we are using here in [its model card](https://huggingface.co/BAAI/bge-large-en-v1.5).\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The string we are embedding is not limited to just a single word. We can pass a string of arbitrary length to the `embed_query` method, but we will always get a single vector of a fixed length back:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_vector = embeddings.embed_query(\n",
    "    \"The tiger, being the largest cat species, is known for its distinctive orange and black stripes, which play a crucial role in its camouflage in the wild.\"\n",
    ")\n",
    "\n",
    "print(embedding_vector[:24], \"...\")\n",
    "print(\"Length of embedding: \", len(embedding_vector))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{hint}\n",
    "\n",
    "Embedding models usually have a maximum number of words (or tokens) that they can consider when calculating the embedding vector. If the string is longer than that, you will see an error. You can find your chosen model's maximum length on its model card. Look for a parameter called _input length_, _sequence length_, or _context length_. In our example here, the [maximum input length is 512 tokens](https://huggingface.co/BAAI/bge-large-en-v1.5).\n",
    "\n",
    "Another important consideration is the semantic specificity of the resultant embedding vector: While every word within the sequence (up to the maximum sequence length) affects the final numbers in the embedding vector, it represents something akin to a \"semantic average\". So the longer the input gets, the less sensitive the embedding is to specific details in the text.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangChain's `TextLoader`, `Document`, and `CharacterTextSplitter` classes\n",
    "\n",
    "The text we want to embed often lives in files of some kind, e.g., text files, Word documents, or PDFs. In LangChain, each of these files is called a _document_ and can be represented in code by a [`Document` object](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html#langchain_core.documents.base.Document). \n",
    "\n",
    "Since loading the files and turning them into `Document` objects is a common pattern, LangChain offers [a collection of _document loaders_](https://python.langchain.com/docs/integrations/document_loaders/) that support a variety of use cases and file formats. For example, if we want to load a simple text file (`*.txt`), we can use the `TextLoader` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "directory_to_file = \"./rag_documents/asteroids.txt\"\n",
    "text_loader = TextLoader(directory_to_file)\n",
    "documents = text_loader.load()\n",
    "print(documents[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now pass the contents of the loaded document to the `embed_documents` method, but we will run into an issue because the text is too long for the chose model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    response = embeddings.embed_documents(documents[0].page_content)\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to split the long text into chunks of 512 tokens. We could create our own loop and process the contents of the document, but fortunately LangChain offers [_text splitters_](https://python.langchain.com/docs/concepts/text_splitters/) that we can use together with the document loader to split the loaded text into sequences of the right length and return each chunk as an individual `Document` objects:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "# Create a text splitter that splits into chunks of 512 tokens\n",
    "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    encoding_name=\"cl100k_base\", chunk_size=512, chunk_overlap=0\n",
    ")\n",
    "\n",
    "# Load the text and split it into Document objects\n",
    "documents = text_loader.load_and_split(text_splitter=text_splitter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{hint}\n",
    "\n",
    "How text gets split into tokens (the _encoding_) differs from model to model. The encoding above is technically only correct for GPT 3.5 and GPT 4, but it is usually close enough to work with other models, too.\n",
    "```\n",
    "\n",
    "Now that we have turned our text into chunks of the correct length, let's embed them all by passing them to the `embed_documents` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_vectors = embeddings.embed_documents([d.page_content for d in documents])\n",
    "\n",
    "print(\n",
    "    f\"Embedding {len(documents)} chunks as {len(embedded_vectors)} vectors with {len(embedded_vectors[0])} dimensions.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary \n",
    "\n",
    "Embeddings are representation of strings as numbers. Using the `embed_query` and `embed_documents` functions, we can get the embeddings of text. This lets us do many exciting operations to represent how different words are related to each other.\n",
    "\n",
    "With `embed_documents` we can take advantage of LangChain's `Document` class to embed content from files. \n",
    "\n",
    "The batch size of the default embedding model is 512 tokens. Using a text splitter can help to ensure the correct sequence length."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
