{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings\n",
    "\n",
    "At its core, a large language model predicts the most likely sequence of words based on an input query. Since machine learning models operate best with numerical data, our first task is to represent words numerically. Words are complex, but we can simplify this by identifying words with similar meanings. For instance, \"stupendous\" and \"nice\" convey similar feelings of positivity, though at different intensities. These are aspects of **sentiment analysis**, which is a significant area in natural language processing.\n",
    "\n",
    "To illustrate, consider two conceptual \"sliders\": one for \"niceness\" and another for \"intensity\". The \"niceness\" settings for \"nice\" and \"stupendous\" might be similar, while their \"intensity\" settings differ. Similarly, words like \"horrible\" and \"terrible\" would have similar settings on both sliders. We can represent any word as a specific configuration of such sliders, adjusting them to find the best match. In practice, these configurations can be represented with embeddings, which typically have many dimensions, such as 1024."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- # How they work \n",
    "\n",
    "There are many different ways to create an embedding. The likelihood of words appearing near each other. For example, words like \"Elizabeth\", \"King\", and \"Buckingham\" are more likely to appear around \"Queen\" than a word like \"bulldozer\". While humans grasp this context naturally, itâ€™s more challenging for computers. Embeddings help tackle this by representing text as numerical values, capturing such contextual relationships. -->\n",
    "\n",
    "TO DO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import find_dotenv, load_dotenv\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating an Embedding\n",
    "\n",
    "Let's find an embedding for a word of our choosing. We will be looking into static embeddings, which are fixed representations of words as vectors from a pre-trained model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_dartmouth.embeddings import DartmouthEmbeddings\n",
    "from langchain_dartmouth.llms import ChatDartmouth\n",
    "from langchain_core.output_parsers import JsonOutputParser, ListOutputParser\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import umap\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model for embeddings is different class from the ones that we have used before. We can see how it's used below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = DartmouthEmbeddings(model_name=\"bge-large-en-v1-5\")\n",
    "\n",
    "tiger = embeddings.embed_query(\"tiger\")\n",
    "print(tiger)\n",
    "print(\"Length of tiger: \", len(tiger))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "**Note:** We see that the word \"tiger\" is represented by a list of 1024 numbers. This means that the numeric representation of the word \"tiger\" consists of 1024 dimensions (or sliders) for this particular embedding model. Other models may use fewer or more numbers to represent text. You can read more about the model we are using [here](https://huggingface.co/BAAI/bge-large-en-v1.5)\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "There are several benefits to having the embedding of a word, a primary one is that it gives us the ability to compare how close two words are in meaning. One way of simpling doing so is by taking the **normalized dot product**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lion = embeddings.embed_query(\"lion\")\n",
    "eggs = embeddings.embed_query(\"eggs\")\n",
    "\n",
    "print(\"Similarity between 'tiger' and 'lion': {:.2f}\".format(np.dot(tiger,lion)/np.linalg.norm(tiger)/np.linalg.norm(lion)))\n",
    "print(\"Similarity between 'tiger' and 'eggs': {:.2f}\".format( np.dot(tiger, eggs)/np.linalg.norm(tiger)/np.linalg.norm(eggs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing an embedding\n",
    "A better way to understand an embedding is to visualize it. Let's generate some random words related to different domains, and find their embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatDartmouth(model_name=\"llama-3-1-8b-instruct\", seed=42, temperature=0.0)\n",
    "parser = JsonOutputParser()\n",
    "\n",
    "chain = llm | parser\n",
    "\n",
    "response = chain.invoke(\n",
    "    \"Generate 30 different words that are well-suited to showcase how word embeddings work. \"\n",
    "    \"Draw the words from domains like animals, finance, and food. The food one should contain tomato \"\n",
    "    \"Return the words in JSON format, using the domain as the key, and the words as values. \"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = pd.DataFrame.from_dict(response).melt(var_name=\"domain\", value_name=\"word\")\n",
    "\n",
    "embeddings = DartmouthEmbeddings(model_name=\"bge-large-en-v1-5\")\n",
    "words[\"embedding\"] = embeddings.embed_documents(words[\"word\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is difficult to visualize a 1024 dimensional vector, as we're not 1024 dimensional humans! One way to get around this is by using a [UMAP](https://umap-learn.readthedocs.io/en/latest/) (Uniform Manifold Approximation and Projection) to represent this large vector as a 2 dimesional one. This can then be plotted as follows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting using UMAP\n",
    "mapper = umap.UMAP().fit(np.array(words[\"embedding\"].to_list()))\n",
    "umap_embeddings = pd.DataFrame(mapper.transform(np.array(words[\"embedding\"].to_list())), columns=[\"UMAP_x\", \"UMAP_y\"])\n",
    "\n",
    "# merge with the words\n",
    "words = pd.concat([words, umap_embeddings], axis=1)\n",
    "words.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a scatter plot using matplotlib\n",
    "for i in words[\"domain\"].unique():\n",
    "    subset = words[words[\"domain\"] == i]\n",
    "    plt.scatter(subset[\"UMAP_x\"], subset[\"UMAP_y\"], label=i)\n",
    "    \n",
    "    # Add the text labels\n",
    "    for j in range(len(subset)):\n",
    "        plt.text(\n",
    "            subset[\"UMAP_x\"].iloc[j],\n",
    "            subset[\"UMAP_y\"].iloc[j] + 0.12,\n",
    "            subset[\"word\"].iloc[j],\n",
    "            horizontalalignment=\"center\",\n",
    "            verticalalignment=\"center\",\n",
    "            fontsize=6,\n",
    "        )\n",
    "\n",
    "# Add legend and show plot\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.title(\"UMAP Projection of Words\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that groups with words related to foods, and animals, and finance are somewhat close to each other. This let's us find the similarity between different words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding a document\n",
    "\n",
    "If there are several words for which we want an embedding, we can use the `embed_documents` command instead. \n",
    "# TOCOMPLETE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_dartmouth.llms import DartmouthLLM\n",
    "\n",
    "llm = ChatDartmouth(model_name=\"llama-3-1-8b-instruct\", seed=42, temperature=0.0)\n",
    "response1 = llm.invoke(\"Generate a 100 word text about dartmouth college and it's history and area\")\n",
    "\n",
    "llm = ChatDartmouth(model_name=\"llama-3-1-8b-instruct\", seed=45, temperature=0.8)\n",
    "response2 = llm.invoke(\"Generate a 100 word text about dartmouth college and it's history and area\")\n",
    "\n",
    "llm = ChatDartmouth(model_name=\"llama-3-1-8b-instruct\", seed=10, temperature=0.0)\n",
    "response3 = llm.invoke(\"Create 5 words of gibberish\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response1.content)\n",
    "print(response2.content)\n",
    "print(response3.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_embeddings(response, upper_limit=400):\n",
    "    words = response.split(\" \")\n",
    "    embedding_list = []\n",
    "    chunks = [words[i:i + 32] for i in range(0, upper_limit, 32)]\n",
    "    for chunk in chunks:\n",
    "        embeddings = DartmouthEmbeddings(model_name=\"bge-large-en-v1-5\")\n",
    "        embedding_list.append(embeddings.embed_documents(chunk))\n",
    "    return np.concatenate(embedding_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using the wikipedia api, we can get some articles from wikipedia and do some semantic analysis on them  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipediaapi\n",
    "\n",
    "def get_wikipedia_page_text(page_title, language=\"en\"):\n",
    "    user_agent = \"CoolBot/0.0 (https://example.org; coolbot@example.org)\"\n",
    "    \n",
    "    wiki_wiki = wikipediaapi.Wikipedia(\n",
    "        language=language,\n",
    "        user_agent=user_agent\n",
    "    )\n",
    "    \n",
    "    # Fetch the page\n",
    "    page = wiki_wiki.page(page_title)\n",
    "    if page.exists():\n",
    "        return page.text\n",
    "    else:\n",
    "        return \"Page not found\"\n",
    "\n",
    "dartmouth_text = get_wikipedia_page_text(\"Dartmouth College\")\n",
    "french_text = get_wikipedia_page_text(\"Claude Cohen-Tannoudji\", \"fr\")\n",
    "Ivy_league_text = get_wikipedia_page_text(\"Ivy League\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dartmouth_embedding = get_embeddings(dartmouth_text)\n",
    "\n",
    "# get embedding of something random \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "french_embedding = get_embeddings(french_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ivy_league_embedding = get_embeddings(Ivy_league_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the centroid of the embeddings\n",
    "dartmouth_centroid = np.mean(dartmouth_embedding, axis=0)\n",
    "french_centroid = np.mean(french_embedding, axis=0)\n",
    "\n",
    "# find the similarity between the centroid and the random word\n",
    "similarity = np.dot(dartmouth_centroid, french_centroid)\n",
    "print(\"Similarity between Dartmouth College and Claude Cohen-Tannoudji: \", similarity.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the centroid of the embeddings\n",
    "dartmouth_centroid = np.mean(dartmouth_embedding, axis=0)\n",
    "Ivy_league_embedding = np.mean(french_embedding, axis=0)\n",
    "\n",
    "similarity = np.dot(dartmouth_centroid, Ivy_league_embedding)/ (np.linalg.norm(dartmouth_centroid) * np.linalg.norm(Ivy_league_embedding))\n",
    "print(\"Similarity between Dartmouth College and Ivy League: \", similarity.round(2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uses"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
