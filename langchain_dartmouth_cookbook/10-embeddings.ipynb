{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings\n",
    "\n",
    "At it's heart a large language model is predicting what is the most likely string of words to come from an input query. Machine learning models struggle to work with data that is not in a numeric format, so our first task is to represent words in a way that can be understood by computers. However, words are complicated. One way we can simplifiy this is to find words which mean similar things. For example, \"stupendous\" and \"nice\" mean similar things (a positive reaction) but at different intensities. \n",
    "\n",
    "Imagine we have 2 knobs we can turn. One represents \"niceness\" and one represents \"intensity\". Based on that, the \"niceness\" knob for \"nice\" and \"stupendous\" might be very similar, but the \"intensity\" might be different. Similarly, we can image that the settings for words like \"horrible\" and \"terrible\" may be similar. With this intuition, we can represent any word as a specific configuration of different knobs, by twisting and turning to get the perfect match. The examples in this sectino have embeddings with 1024 dimensions (or 1024 different knobs).  \n",
    "\n",
    "Another way is if we can find a word, such as \"Queen\", we can calculate how probable is it for another word to be around it. For example, \"Elizabeth\", \"King\", \"Buckingham\" may all be words that are more likely to appear around the word \"Queen\" compared to something like \"bulldozer\". As humans, we understand this intuitively. For computers, it may be a lot more difficult. One way to deal with this issue is by looking at \"Embeddings\", which are a way to representing text as numbers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import find_dotenv, load_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating an Embedding\n",
    "\n",
    "Let's find an embedding for a word of our choosing. We will be looking into static embeddings, which are embeddings which have been already assigned to several words already. Through different algorithms that analyze the probability of a word given it's semantic meaning and the context around it, each word is given a specific set of numbers, otherwise known as a \"Vector\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_dartmouth.embeddings import DartmouthEmbeddings\n",
    "from langchain_dartmouth.llms import ChatDartmouth\n",
    "\n",
    "from langchain_core.output_parsers import JsonOutputParser, ListOutputParser\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import umap\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = DartmouthEmbeddings()\n",
    "tiger = embeddings.embed_query(\"tiger\")\n",
    "print(tiger)\n",
    "print(\"Length of tiger: \", len(tiger))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the word \"tiger\" is represented by 1024 numbers. This means that the numeric representation of the word \"tiger\" consists of 1024 dimensions for this particular embedding model. Other models may use fewer or more numbers to represent a word. \n",
    "\n",
    "There are several benefits to having the embedding of a word, a primary one is that it gives us the ability to compare how close two words are in meaning. One way of simpling doing so is by taking the dot product. For example: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lion = embeddings.embed_query(\"lion\")\n",
    "eggs = embeddings.embed_query(\"eggs\")\n",
    "\n",
    "print(\"Similarity between tiger and lion: \", np.dot(tiger, lion).round(2))\n",
    "print(\"Similarity between tiger and eggs:\", np.dot(tiger, eggs).round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding a query\n",
    "A better way to understand an embedding is to visualize it. Let's generate some random words related to different domains, and find their embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatDartmouth(model_name=\"llama-3-1-8b-instruct\", seed=42, temperature=0.0)\n",
    "parser = JsonOutputParser()\n",
    "\n",
    "chain = llm | parser\n",
    "\n",
    "response = chain.invoke(\n",
    "    \"Generate 30 different words that are well-suited to showcase how word embeddings work. \"\n",
    "    \"Draw the words from domains like animals, finance, and food. The food one should contain tomato \"\n",
    "    \"Return the words in JSON format, using the domain as the key, and the words as values. \"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = pd.DataFrame.from_dict(response).melt(var_name=\"domain\", value_name=\"word\")\n",
    "\n",
    "embeddings = DartmouthEmbeddings(model_name=\"bge-large-en-v1-5\")\n",
    "words[\"embedding\"] = embeddings.embed_documents(words[\"word\"])\n",
    "\n",
    "print(len(words[\"embedding\"][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is difficult to visualize a 1024 dimensional vector, as we're not 1024 dimensional humans! One way to get around this is by using a UMAP (Uniform Manifold Approximation and Projection) to represent this large vector as a 2 dimesional one. This can then be plotted as follows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapper = umap.UMAP().fit(np.array(words[\"embedding\"].to_list()))\n",
    "\n",
    "umap_embeddings = pd.DataFrame(mapper.transform(np.array(words[\"embedding\"].to_list())), columns=[\"UMAP_x\", \"UMAP_y\"])\n",
    "# merge with the words\n",
    "words = pd.concat([words, umap_embeddings], axis=1)\n",
    "words.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "for i in words[\"domain\"].unique():\n",
    "    sns.scatterplot(data=words[words[\"domain\"] == i], x=\"UMAP_x\", y=\"UMAP_y\", label=i)\n",
    "    # add the text labels\n",
    "    for j in range(len(words)):\n",
    "        plt.text(\n",
    "            words[\"UMAP_x\"].iloc[j],\n",
    "            words[\"UMAP_y\"].iloc[j] + 0.13,\n",
    "            words[\"word\"].iloc[j],\n",
    "            horizontalalignment=\"center\",\n",
    "            verticalalignment=\"center\",\n",
    "            fontsize=7,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that groups with words related to foods, and animals, and finance are somewhat close to each other. This let's us find the similarity between different words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding a document\n",
    "\n",
    "We can also embed an entire document \n",
    "# TOCOMPLETE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_dartmouth.llms import DartmouthLLM\n",
    "\n",
    "llm = ChatDartmouth(model_name=\"llama-3-1-8b-instruct\", seed=42, temperature=0.0)\n",
    "response1 = llm.invoke(\"Generate a 100 word text about dartmouth college and it's history and area\")\n",
    "\n",
    "llm = ChatDartmouth(model_name=\"llama-3-1-8b-instruct\", seed=45, temperature=0.8)\n",
    "response2 = llm.invoke(\"Generate a 100 word text about dartmouth college and it's history and area\")\n",
    "\n",
    "llm = ChatDartmouth(model_name=\"llama-3-1-8b-instruct\", seed=10, temperature=0.0)\n",
    "response3 = llm.invoke(\"Create 5 words of gibberish\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response1.content)\n",
    "print(response2.content)\n",
    "print(response3.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_embeddings(response, upper_limit=400):\n",
    "    words = response.split(\" \")\n",
    "    embedding_list = []\n",
    "    chunks = [words[i:i + 32] for i in range(0, upper_limit, 32)]\n",
    "    for chunk in chunks:\n",
    "        embeddings = DartmouthEmbeddings(model_name=\"bge-large-en-v1-5\")\n",
    "        embedding_list.append(embeddings.embed_documents(chunk))\n",
    "    return np.concatenate(embedding_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipediaapi\n",
    "\n",
    "def get_wikipedia_page_text(page_title, language=\"en\"):\n",
    "    # Create a user-agent string\n",
    "    user_agent = \"CoolBot/0.0 (https://example.org; coolbot@example.org)\"\n",
    "    \n",
    "    # Initialize Wikipedia API with the user agent\n",
    "    wiki_wiki = wikipediaapi.Wikipedia(\n",
    "        language=language,\n",
    "        user_agent=user_agent\n",
    "    )\n",
    "    \n",
    "    # Fetch the page\n",
    "    page = wiki_wiki.page(page_title)\n",
    "    \n",
    "    if page.exists():\n",
    "        return page.text\n",
    "    else:\n",
    "        return \"Page not found\"\n",
    "\n",
    "# Example usage\n",
    "page_title = \"Dartmouth College\"\n",
    "dartmouth_text = get_wikipedia_page_text(page_title)\n",
    "french_text = get_wikipedia_page_text(\"Claude Cohen-Tannoudji\", \"fr\")\n",
    "Ivy_league_text = get_wikipedia_page_text(\"Ivy League\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dartmouth_embedding = get_embeddings(dartmouth_text)\n",
    "\n",
    "# get embedding of something random \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "french_embedding = get_embeddings(french_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ivy_league_embedding = get_embeddings(Ivy_league_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the centroid of the embeddings\n",
    "dartmouth_centroid = np.mean(dartmouth_embedding, axis=0)\n",
    "french_centroid = np.mean(french_embedding, axis=0)\n",
    "\n",
    "# find the similarity between the centroid and the random word\n",
    "similarity = np.dot(dartmouth_centroid, french_centroid)\n",
    "print(\"Similarity between Dartmouth College and Claude Cohen-Tannoudji: \", similarity.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the centroid of the embeddings\n",
    "dartmouth_centroid = np.mean(dartmouth_embedding, axis=0)\n",
    "Ivy_league_embedding = np.mean(french_embedding, axis=0)\n",
    "\n",
    "similarity = np.dot(dartmouth_centroid, Ivy_league_embedding)/ (np.linalg.norm(dartmouth_centroid) * np.linalg.norm(Ivy_league_embedding))\n",
    "print(\"Similarity between Dartmouth College and Ivy League: \", similarity.round(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# given these embeddings we can now use them to find the similarity between the two documents\n",
    "# we can also use them to find the similarity between the two documents and a random word\n",
    "\n",
    "# get the centroid of the embeddings\n",
    "dartmouth_centroid = np.mean(dartmouth_embedding, axis=0)\n",
    "french_centroid = np.mean(french_embedding, axis=0)\n",
    "\n",
    "# find the similarity between the centroid and the random word\n",
    "similarity = np.dot(dartmouth_centroid, french_centroid)\n",
    "print(\"Similarity between Dartmouth College and Claude Cohen-Tannoudji: \", similarity.round(2))\n",
    "\n",
    "dartmouth_board_centroid = np.mean(dartmouth_board_embedding, axis=0)\n",
    "\n",
    "similarity = np.dot(dartmouth_centroid, dartmouth_board_centroid)\n",
    "print(\"Similarity between Dartmouth College and Dartmouth Board: \", similarity.round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uses"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
