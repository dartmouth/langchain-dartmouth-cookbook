{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conversational Memory\n",
    "\n",
    "As we mentioned in previous recipes, large language models have no internal state, i.e., they do not retain any conversational context from previous messages. A multi-turn conversation works by passing an increasingly longer prompt to the model that includes all previous messages in addition to the most recent one. There are several ways to manage the conversational context, or conversational memory, which have their individual strenghts and weaknesses. In this recipe, we will explore some of the most common ones.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import find_dotenv, load_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_dartmouth.llms import ChatDartmouth\n",
    "\n",
    "llm = ChatDartmouth(model_name=\"llama-3-1-8b-instruct\", temperature=0, seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-persistent conversational memory \n",
    "\n",
    " We can think of the conversational memory as the history of all messages that have been passed to and received from the model so far. In [Prompt Basics](06-prompt-basics.ipynb), we saw that we can pass a list of messages to a chat model. We can use this mechanism to create a simple conversational memory system by appending every message (outgoing and incoming) to a list:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "first_message = HumanMessage(\"Ask me a riddle!\")\n",
    "conversation = [first_message]\n",
    "\n",
    "first_response = llm.invoke(conversation)\n",
    "conversation.append(first_response)\n",
    "\n",
    "for message in conversation:\n",
    "    message.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_message = HumanMessage(\"Is it a unicorn?\")\n",
    "conversation.append(second_message)\n",
    "\n",
    "second_response = llm.invoke(conversation)\n",
    "conversation.append(second_response)\n",
    "\n",
    "for message in conversation:\n",
    "    message.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While this technique works for relatively simple scenarios, it's not very elegant and requires quite a bit of code to maintain the history. It also can be potentially problematic when the LLM is part of a chain and we don't want to pass the conversation history as an input to the chain.\n",
    "\n",
    "Instead, the LLM component should keep track of the message history internally!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fortunately, LangChain offers a way to make that happen. We need two things for this:\n",
    "- a component that keeps track of the message history (replacing the simple list above)\n",
    "- a way for the LLM to interact with this list whenever a new (input or output) message arrives (replacing the list management code we wrote above)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To keep track of the message history, we can use a class called `ChatMessageHistory`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "\n",
    "history = ChatMessageHistory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This component works very similarly to the simple list we used above, but is more explicitly designed to be used with messages. For example, here is how we create the history from above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.add_message(first_message)\n",
    "history.add_message(first_response)\n",
    "history.add_message(second_message)\n",
    "history.add_message(second_response)\n",
    "\n",
    "history.messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make an LLM use a `ChatMessageHistory` object, we need to \"attach\" it to the `ChatDartmouth` component by wrapping them with a class called `RunnableWithMessageHistory`. \n",
    "\n",
    "This class assumes that we want to be able to manage multiple conversation histories, as we would in a chat application. It therefore expects a function that returns a chat message history object given a session id. In this example, we only keep track of a single conversation, so we can just return the same history every time. So we just need to write a very simple dummy function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = ChatMessageHistory()\n",
    "\n",
    "\n",
    "def get_history(session_id):\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "**Note:** We have to make sure to instantiate the history outside the function. Otherwise, the message history would not persist between calls to `get_history`!</div>\n",
    "\n",
    "Now we have everything we need to tie it all together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "\n",
    "llm_with_memory = RunnableWithMessageHistory(\n",
    "    runnable=llm,\n",
    "    get_session_history=get_history,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "**Note:** LangChain calls all components that implement the standard interface of the `invoke` and `stream` methods (and some others) a _runnable_. </div>\n",
    "\n",
    "When we invoke this runnable, we have to specify the session id that will be passed to `get_history` (even though we don't use it here):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_with_memory.invoke(\n",
    "    {\n",
    "        \"input\": \"Tell me a riddle!\",\n",
    "    },\n",
    "    config={\"configurable\": {\"session_id\": \"whatever\"}},\n",
    ").pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_with_memory.invoke(\n",
    "    {\"input\": \"Give me a hint\"},\n",
    "    config={\"configurable\": {\"session_id\": \"whatever\"}},\n",
    ").pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_with_memory.invoke(\n",
    "    {\"input\": \"Is it a river?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"whatever\"}},\n",
    ").pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the message history object to see that indeed keeps track of all the messages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for message in history.messages:\n",
    "    message.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks great, doesn't it?\n",
    "\n",
    "One issue remains, however: Depending on our use case, we might want to persist the message history between runs of the program. Or maybe we want to be able to do something more meaningful with the session id in `get_history`, e.g., manage multiple conversations. In the next section, we will learn about ways to achieve both of those things!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persistent conversational memory\n",
    "\n",
    "While we could write the message history to disk at the end of every run of our program and read it back in at the start of each run, that would be a bit cumbersome and would require additional boilerplate code. We also might want to consider different options to store the history, like a SQL database.\n",
    "\n",
    "LangChain offers a variety of implementations for the [message history](https://api.python.langchain.com/en/latest/community_api_reference.html#module-langchain_community.chat_message_histories), built on different services. For example, we can store the history to a SQLite database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import SQLChatMessageHistory\n",
    "\n",
    "DB_NAME = \"chat_history.db\"\n",
    "\n",
    "\n",
    "def get_history(session_id):\n",
    "    return SQLChatMessageHistory(session_id, connection=f\"sqlite:///{DB_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now every time we call `get_history`, the chat history will be retrieved from the specified SQLite database, using the session ID as a filter.\n",
    "\n",
    "We can now manage multiple conversations by specifying a separate ID for each conversation thread:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_with_memory = RunnableWithMessageHistory(\n",
    "    runnable=llm,\n",
    "    get_session_history=get_history,\n",
    ")\n",
    "\n",
    "llm_with_memory.invoke(\n",
    "    {\"input\": \"Hi, I am Simon!\"},\n",
    "    config={\"configurable\": {\"session_id\": \"simons_convo\"}},\n",
    ").pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_with_memory.invoke(\n",
    "    {\"input\": \"Hi, I am Alex!\"},\n",
    "    config={\"configurable\": {\"session_id\": \"alex_convo\"}},\n",
    ").pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can inspect the database like any other SQLite database, e.g. with Python's built-in `sqlite3` module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "con = sqlite3.connect(DB_NAME)\n",
    "cur = con.cursor()\n",
    "\n",
    "# By default, the table name is 'message_store'\n",
    "cur.execute(\"SELECT * FROM message_store;\").fetchall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the conversations are organized by the session ID, so we can continue to have separate conversations by passing the respective session ID:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_with_memory.invoke(\n",
    "    {\"input\": \"What's my name again?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"simons_convo\"}},\n",
    ").pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_with_memory.invoke(\n",
    "    {\"input\": \"What's my name again?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"alex_convo\"}},\n",
    ").pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the database is stored on disk by default, the history is automatically persisted across multiple runs of the program. If you want to use one of the [other implementations ](https://api.python.langchain.com/en/latest/community_api_reference.html#module-langchain_community.chat_message_histories)of the chat message history, you only need to change the `get_history` function!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "LLMs are stateless and thus require the entire conversation to generate the next turn. We can keep track of the conversation manually by maintaining a list of all outgoing and incoming messages. If we want a more elegant solution that can optionally persist the message history using a variety of backends (e.g., a SQL database), we can use one of [LangChain's implementations of the chat message history](https://api.python.langchain.com/en/latest/community_api_reference.html#module-langchain_community.chat_message_histories)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
